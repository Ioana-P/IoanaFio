<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="Breaking down the first part of transformer language models - the self-attention mechanism." />

  
  <link rel="alternate" hreflang="en-us" href="/publication/self_attention_1/" />

  
  
  
    <meta name="theme-color" content="rgb(251, 191, 183)" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.3a1b2f5c1da49b147f3050c4749cae46.css" />

  



  


  


  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="/publication/self_attention_1/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@wowchemy" />
    <meta property="twitter:creator" content="@wowchemy" />
  
  <meta property="og:site_name" content="Academic" />
  <meta property="og:url" content="/publication/self_attention_1/" />
  <meta property="og:title" content="1 | Basics of Self-Attention | Academic" />
  <meta property="og:description" content="Breaking down the first part of transformer language models - the self-attention mechanism." /><meta property="og:image" content="/publication/self_attention_1/featured.png" />
    <meta property="twitter:image" content="/publication/self_attention_1/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2020-07-01T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2020-07-01T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/publication/self_attention_1/"
  },
  "headline": "1 | Basics of Self-Attention",
  
  "image": [
    "/publication/self_attention_1/featured.png"
  ],
  
  "datePublished": "2020-07-01T00:00:00Z",
  "dateModified": "2020-07-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Ioana Fiona Preoteasa"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Academic",
    "logo": {
      "@type": "ImageObject",
      "url": "/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Breaking down the first part of transformer language models - the self-attention mechanism."
}
</script>

  

  

  

  





  <title>1 | Basics of Self-Attention | Academic</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="61848222bdf68538a25bbe58706a7fb1" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.d221902fdb3ac4942841d39f708dccc2.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/post/"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/DS_cv_resume.pdf"><span>CV</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link  active" href="/publication/"><span>Writing</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    








<div class="pub">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>1 | Basics of Self-Attention</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      Ioana Fiona Preoteasa</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July, 2020
  </span>
  

  

  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    

    
    

    

    <div class="space-below"></div>

    <div class="article-style"><p>TL;DR — Transformers are an exciting and (<strong>relatively</strong>) new part of Machine Learning (ML) but there are a <strong>lot</strong> of concepts that need to be broken down before you can understand them. This is the first post in a column I’m writing about them. Here we focus on how the basic self-attention mechanism works, which is the first layer of a Transformer model. Essentially for each input vector Self-Attention produces a vector that is the weighted sum over the vectors in its neighbourhood. The weights are determined by the relationship or <em>connectedness</em> between the words. This column is aimed at ML novices and enthusiasts who are curious about what goes on under the hood of Transformers.</p>
<h1 id="contents">Contents:</h1>
<ol>
<li><a href="#cce2">Introduction</a></li>
<li><a href="#2beb">Self-Attention — the math</a></li>
<li><a href="#c2e8">References</a></li>
</ol>
<h1 id="1-introduction">1. Introduction</h1>
<p>Transformers are an ML architecture that have been used successfully in a wide variety of NLP tasks, especially sequence to sequence (seq2seq) ones such as machine translation and text generation. In seq2seq tasks, the goal is to take a set of inputs (e.g. words in English) and produce a desirable set of outputs (- the same words in German). Since their inception in 2017, they’ve usurped the dominant architecture of their day (<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="noopener">LSTMs</a>) for seq2seq and have become almost ubiquitous in any news about NLP breakthroughs (for instance OpenAI’s <a href="https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it" target="_blank" rel="noopener">GPT-2 even appeared in mainstream</a> media!).</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/984/1*pblofc3psQrBkvXI4Jfxog.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 1.1 — machine translation (EN → DE)⁴</p>
<p>This column is intended as a very gentle, gradual introduction to the math, code and concept behind Transformer architecture. There’s no better place to start with than the attention mechanism because:</p>
<blockquote>
<p>The most basic transformers rely purely on attention <strong>mechanisms³.</strong></p>
</blockquote>
<h1 id="2-self-attention--the-math">2. Self-Attention — the math</h1>
<p>We want an ML system to learn the important relationships between words, similar to the way a human being understands words in a sentence. In Fig 2.1 you and I both know that “The” is referring to “animal” and thus should have a strong connection with that word. As the diagram’s colour coding shows, this system knows that there is some connection between “animal”, “cross”,“street” and “the” because they’re all <em>related</em> to “animal”, the subject of the sentence. This is achieved through <em>Self-Attention.⁴</em></p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1400/1*9XxSNAGInd3rbwTE_AwrQA.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.1 — which words does “The” pay <strong><em>attention</em></strong> to?⁴</p>
<p>At its most basic level, Self-Attention is a process by which one sequence of vectors <em>x</em> is <strong>encoded</strong> into another sequence of vectors <em>z</em> (Fig 2.2). Each of the original vectors is just a <strong>block of numbers</strong> that <strong>represents a word.</strong> Its corresponding <em>z</em> vector represents both the original word <em>and</em> its <strong>relationship</strong> with the other words around it.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1400/1*qeY6mWlzwkCIl2LhPN0zZQ.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.2: sequence of input vectors <em>x</em> getting turned into another equally long sequence of vectors <em>z</em></p>
<p>Vectors represent some sort of thing in a <em>space,</em> like the flow of water particles in an ocean or the effect of gravity at any point around the Earth. You <em>can</em> think of words as vectors in the total space of words. The direction of each word-vector <em>means</em> something. Similarities and differences between the vectors correspond to similarities and differences between the words themselves (I’ve written about the subject before <a href="https://medium.com/analytics-vidhya/ideas-for-using-word2vec-in-human-learning-tasks-1c5dabbeb72e" target="_blank" rel="noopener">here</a>).</p>
<p>Let’s just start by looking at the first three vectors and only looking in particular at how the vector <em>x2</em>, our vector for “cat”, gets turned into <em>z2</em>. All of these steps will be repeated for <em>each</em> of the input vectors.</p>
<p>First, we multiply the vector in our spotlight, <em>x2</em>, with all the vectors in a sequence, <em>including itself</em>. We’re going to do a product of each vector and the <em>transpose</em> (the diagonally flipped version) of <em>x2</em> (Fig 2.3). This is the same as doing a dot product and you can think of a dot product of two vectors as a measure of <strong>how similar they are.</strong></p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1400/1*dVJGPnBgZAFy8MorveslUQ.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.3: transposed multiplication (superscript “T” = “transposed”)</p>
<p>The dot product of two vectors is proportional to the cosine of the angle between them (Fig 2.4) — so the more closely they align in direction, the larger the dot product. If they were pointing in the exact same direction then the angle A would be 0⁰ and a cosine of 0⁰ is equal to 1. If they were pointing in opposite directions (so that A = 180⁰) then the cosine would be -1.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1400/1*2c4vsG2yNRBQL8xsIYKuew.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.4 — dot product of two vectors</p>
<p>As an aside, note that the <em>operation</em> we use to get this product between vectors is a hyperparameter we can choose. The dot product is just the simplest option we have and the one that’s used in <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener"><em>Attention Is All You Need</em></a><em>³</em> (AIAYN)<em>.</em></p>
<p>If you want an additional intuitive perspective on this, <a href="http://www.peterbloem.nl/blog/transformers" target="_blank" rel="noopener">Bloem’s</a>¹ post discusses how self-attention is analogous to the way recommender systems determine the similarity of movies or users.</p>
<p>So we put one word under the spotlight at a time and determine its output from its neighbourhood of words. Here we’re only looking at the words before and after but we could choose to widen that window in the future.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/900/1*RN9sHNRPhQu2atGXzTW5zg.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.5 — raw weights for each j-th vector</p>
<p>If the spotlit word is “cat”, the sequence of words we’re going over is “the”, “cat”, “sat”. We’re asking <strong>how much attention the word “<em>cat”</em> should pay to “<em>the”, “cat”</em> and “<em>sat” respectively</em></strong> (similar to what we see in Fig 2.1).</p>
<p>Multiplying the transpose of our spotlit word vector and the sequence of words around it will give us a set of 3 <em>raw weights</em> (Fig 2.5)<em>.</em> Each weight is proportional to how connected the two words are in meaning. We need to then normalise them so they are easier to use going ahead. We’ll do this using the <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener">softmax formula (Fig 2.6).</a> This converts a sequence of numbers to be within the range of 0, 1 where each output is proportional to the <em>exponential of the input number</em>. This makes our weights much easier to use and interpret.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1068/1*FM5PaDrHI31yoE8AwvMAWw.jpeg" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.6: normalising raw weights via softmax function</p>
<p>Now we take our normalised weights (one per every vector in the <em>j</em> sequence), multiply them respectively with the <em>x</em> input vectors, sum the products and bingo! We have an output <em>z</em> vector, (Fig 2.5)! This is, of course, the output vector <strong>just</strong> for x2 (“cat”) — this operation will be repeated for every input vector in <em>x</em> until we get the output sequence we saw in Fig 2.2.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1400/1*Q1d4gzdBleLgcMUrI58D8g.jpeg" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.7: Final operation to get our new sequence of vectors, <em>z</em></p>
<p>This explanation so far may have raised some questions:</p>
<ul>
<li>Aren’t the weights we calculated highly dependent on how we determined the original input vectors?</li>
<li>Why are we relying on the <em>similarity</em> of the vectors? What if we want to find a connection between two ‘dissimilar’ words, such as the object and subject of “The cat sat on the matt”?</li>
</ul>
<p>In the next post, we’ll address these questions. We’ll transform each vector for each of its different uses and thus define relationships between words <em>more precisely</em> so that we can get an output more like Fig 2.8.</p>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="https://miro.medium.com/max/1400/1*al_9j5AzCoqPaTUMjFRkjQ.png" alt="" loading="lazy" data-zoomable /></div>
  </div></figure>
Fig 2.8 — which words is “cross” paying attention to in the <strong>orange column</strong> vs the <strong>pink</strong> one?</p>
<p>I hope you’ve enjoyed this post and I appreciate any amount of claps. Feel free to leave any feedback (positive or constructive) in the comments and I’ll aim to take it onboard as quickly as I can.</p>
<p>The people who helped my understanding the most and to whom I am very grateful are Peter Bloem (his <a href="http://www.peterbloem.nl/blog/transformers" target="_blank" rel="noopener">post</a> is a great start if, like me, you prefer a math-first approach to Machine Learning¹ ) and Jay Alammar (if you want a top-down view to start with, I recommend <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">his article</a>²).</p>
<h1 id="3-references">3. References</h1>
<ol>
<li>Alammar J. <em>The Illustrated Transformer.</em> (2018)  <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a> [accessed 27th June 2020]</li>
<li>Bloem P. <em>Transformers from Scratch.</em> (2019) <a href="http://www.peterbloem.nl/blog/transformers" target="_blank" rel="noopener">http://www.peterbloem.nl/blog/transformers</a> .[accessed 27th June 2020]</li>
<li>Vaswani A. et al. Dec 2017. <em>Attention is all you need</em>. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a> [accessed 27th June 2020]. <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">arXiv:1706.03762</a></li>
<li>Vaswani A. et al. Mar 2018 <a href="https://arxiv.org/abs/1803.07416" target="_blank" rel="noopener">arXiv:1803.07416</a> . <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?authuser=2#scrollTo=OJKU36QAfqOC" target="_blank" rel="noopener">Interactive notebook</a>: [accessed 29th June 2020]</li>
</ol>
</div>

    




<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/nlproc/">NLProc</a>
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/publication/self_attention_1/&amp;text=1%20%7c%20Basics%20of%20Self-Attention" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/publication/self_attention_1/&amp;t=1%20%7c%20Basics%20of%20Self-Attention" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=1%20%7c%20Basics%20of%20Self-Attention&amp;body=/publication/self_attention_1/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/publication/self_attention_1/&amp;title=1%20%7c%20Basics%20of%20Self-Attention" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=1%20%7c%20Basics%20of%20Self-Attention%20/publication/self_attention_1/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/publication/self_attention_1/&amp;title=1%20%7c%20Basics%20of%20Self-Attention" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu7eb9c9766accbf54f7f04ff419d2cc82_1157039_270x270_fill_q75_lanczos_center.jpg" alt="Ioana Fiona Preoteasa">
    

    <div class="media-body">
      <h5 class="card-title">Ioana Fiona Preoteasa</h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Ioana-P" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/ioanapr/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
















  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2022 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.e66e385e2f1df861699d60acd7a9c670.js"></script>

    
    
    
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.ab2f2890dbe3e2e83579366d3d6e8fd9.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>






</body>
</html>
