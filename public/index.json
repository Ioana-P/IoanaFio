[{"authors":null,"categories":null,"content":"Ioana (she/her) is a data scientist and former secondary school teacher. Combining analytical rigour \u0026amp; pedagogical expertise to turn opaque data and ML methods into insights \u0026amp; narrative. Note: this site is very new and am still in the process of adding much of my projects and content from other places.\nDownload my CV.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Ioana (she/her) is a data scientist and former secondary school teacher. Combining analytical rigour \u0026 pedagogical expertise to turn opaque data and ML methods into insights \u0026 narrative. Note: this site is very new and am still in the process of adding much of my projects and content from other places.","tags":null,"title":"Ioana Fiona Preoteasa","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"Using OSINT tools and Transformers to extract topics and sentiment Using the Blattodea tool that I helped develop during a hackathon, I retrieved the most recent tweets from Elon Musk. I then used one of HuggingFace‚Äôs pre-trained sentiment classification models and BERTopic to extract and visualize key themes. I have also developed an RShiny dashboard for this project to hone my interactive visualization skills.\nNote on utility and future projects BERTopic can accommodate ‚Äúonline topic modelling‚Äù (i.e. incrementally adjusts topics with new data), and the results in this project have shown the model to be qualitatively coherent (lacking a labelled dataset, I am unable to compute the exact accuracy of clustering/classification). It would not be difficult to expand this work to a more regular, (semi-)automated pipeline to monitor social media content around a particular hashtag/person/theme and to extract insight or detect sudden changes. Suppose you were a news organisation looking to gauge interest in a particular recent event. Although social media isn‚Äôt representative of general discourse around any given topic, taking data from Twitter, passing it through BERTopic and then setting the pipeline to regularly update the data and topics, would give you the data to assess at least some of the discussion around a theme or event.\nResults and thoughts (Results are analysed and visualized at greater length inside the repo‚Äôs index.Rmarkdown notebook. For cleaning, EDA and modelling code, please see the Jupyter notebooks in the repo, explained in the filing system below)\nOverall usage I have been able to extract clear and definite topics from the collected data, and the pattern of activity around key themes has been what I expected it to be. For example, Musk‚Äôs opining on ending the war in Ukraine generated a larger amount of responses across the board than his other tweets. Through this project I‚Äôve found that BERTopic has been extremely useful and capable of extracting information from unstructured text data, and I plan on using it in future projects. Moreover, the model was able to group topic clusters at a greater level of precision and accuracy than I had honestly expected. In the dendrogram included below I show how some of the topics (the most interesting ones) have been grouped by the model and what the model tells us vs what we can infer. For an NLP project is arguably even more important than in most data science projects to combine contextual knowledge and data viz with the results: language is far more ambiguous and mysterious than numbers. Note that the tweets analysed here are all from 1st August 2022 onwards.\nClustering Topics Given the time-range for our data, it should be no surprise that tweets about the war have formed their own distinct group of clusters. It‚Äôs easy to see how topics 35, 36, 1 and 10 are linked. For interpreting this graph, recall that the most important number to count is the number of steps to take along the tree for two topics/leafs to connect. (Note that these are graphs generated with the help of BERTopic and there‚Äôs far less you can do in terms of graph customization).\nTopics 29 and 9 form an understandable cluster together as there were a significant number of tweets focused on Starlink‚Äôs activity in Ukraine (and its commercial activity more generally), both by Musk and his followers. The grouping of clusters 3 and 2 is more interesting: closer inspection of 2 revealed that it includes some tweets related to Twitter bots (something Musk has made a point of discussing openly recently), which would link it sensibly to topic 3; however some of the tweets were also referring to Tesla ‚Äòbots‚Äô (i.e. Tesla‚Äôs robotics research and department). If we‚Äôd had a set of topic labels for each of these, it‚Äôs very likely that BERTopic would misclassify the tweets in this particular topic. Topic two tweets range from :\n| ‚ÄúTesla Bot is the future of robotics ü§Ø \u0026#34;\nto\n| ‚ÄúIf Twitter put as much effort into botspam removal as they do into subpoenas we wouldnt have this problem in the first place‚Äù\nHuman language users like us can tell that these are two distinct themes. Yet I‚Äôd say that this amalgamation of is quite an understandable mistake, given the amount of words that appear across topic 2 that make it strongly related to topic 3 and others. It is possible that if we had more documents, BERTopic would‚Äôve split topic 2 into another leaf.\nTowards the bottom we can also see three topics that, although not as closely aligned as the war-themed ones, still cluster together. These are all connected by the theme of rocketry, SpaceX and all of Musk‚Äôs space-related endeavours. It‚Äôs encouraging to see that the model was able to place these closely together. Note the next leaf that joins this particular sub-branch, topic 55, related to engines. Now, when I originally looked at this topic, I figured that the algorithm had misgrouped again and that discussions of engines must be focussed on cars. However, I ‚Ä¶","date":1666051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666051200,"objectID":"0c4631ef03ecd73302541424a85e5087","permalink":"/project/twitter_sentiment_tracking/","publishdate":"2022-10-18T00:00:00Z","relpermalink":"/project/twitter_sentiment_tracking/","section":"project","summary":"Using OSINT tools and Transformers to extract topics and sentiment from Elon Musk's corner of the Twitter-sphere.","tags":["Deep Learning","NLProc","Unsupervised-Learning","Machine Learning"],"title":"Visualizing and quantifying topics on Twitter","type":"project"},{"authors":["Ioana Fiona Preoteasa","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more The template is mobile first with a responsive design to ensure that your site looks stunning on every device. Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Ioana Fiona Preoteasa"],"categories":null,"content":"Picture a novice data scientist, either straight out of education or pivoting careers. They are determined to enter the field but have limited time and funds to spare in extra training and are daunted by the sheer variety of skills and experiences employers request. Naturally, they sign up to intense bootcamps and scramble to take online courses on everything from Azure to Excel.\nHow can you be sure that focusing on a particular skill won‚Äôt prove to be a massive opportunity cost? How many years experience are they expecting? What should I expect to get paid, realistically? If you‚Äôre leafing through a data science syllabus, and you want to make sure your investment pays off then what should that syllabus include? That‚Äôs also a very pertinent question for people designing a data science course: how do we maximise the overlap between what recruiters want and what our course teaches?\nEvery new or aspiring data scientist right now‚Äî Img 1.1 ‚Äî Photo courtesy of UnSplash\nAbout this post I‚Äôm reporting on findings from an exploration of job data on Indeed.co.uk, so this focusses on the UK job market. Although this post provides useful answers, there are still plenty of questions to ask and it‚Äôs unclear how representative the findings are of the total population of jobs. This is why future iterations of the project in Q1 2021 and later will try to replicate / falsify the findings. If you have suggestions for other things to search for within the data or any critiques of methods used, please leave a comment. All feedback is appreciated.\nIf you just want the most important results, scroll to section II. Key insights.\nIf you want to know how those results were generated in more detail, scroll to section III. Methodology.\nIf you want to be able to replicate the findings, read the assumptions or see the results in full (including more of the null results), see the README in the project repo and the main notebook.\nI. Why and How Harvard Business Review dubbed Data Scientist the sexiest job of the century, and the rapid wave of data bootcamps and online courses over the past decade reflects the immense magnetism of the profession. Yet with so many people rushing into data science, it‚Äôs important to know the market and offer a competitive resume. This post and its corresponding project repository represent my initiative in adding to the trove of knowledge on the job market. This is an exploratory, single-researcher analysis, using data scraped only from Indeed, with a total sample of 1082 job descriptions and 382 annual salaries. Therefore you should take findings with a pinch of salt. The plan is to replicate the scraping and analysis during Q1 2021. I plan to broaden the range of analyses performed on the data as well as improve the quality and efficiency of the web-scraping tool. I searched for jobs with 3 different pairs of words in the title:\ndata, scientist (DS) machine, learning (ML) data, analyst (DA) I‚Äôve included all of those under the umbrella of ‚Äúdata science roles‚Äù because even though it‚Äôs common knowledge that those roles do very different things and require different proficiencies, they still all do data science, and the lines between the categories is blurry.\nWho does this analysis benefit? Who are potential stakeholders? Let‚Äôs imagine that I am an aspiring data scientist, recently starting out in the field. Regardless of my current qualifications, I want to know what employers want so I know what skills I need to go and acquire, which of my skills I can best leverage, and what I else I should learn over time to increase my potential salary. Let‚Äôs suppose that we are a data science course provider / bootcamp based in the UK. Our bottom line is getting our learners hired in the data science world and, additionally, trying to maximise the average salaries our graduates get. To do so, we need to match our curriculum to what the market is asking for. What‚Äôs the right combination of skills, programming languages and expertise that we should be delivering? That‚Äôs a question we can answer by looking at employer needs. II. Key insights 1. 3 out of 5 data science roles do not state salary openly (~59%): The discrepancy is consistent across the three categories, though wider for roles with ‚Äúmachine learning‚Äù (ML) in the title than those with ‚Äúdata analyst‚Äù (DA). This is particularly daunting for the lone, fresh data scientist entering the field, especially if they have no prior experience of negotiating pay in any field.\nIf you‚Äôre an aspiring data scientist: use Glassdoor to research salaries at a company (where possible) and always bear in mind the average salary for that type of role you‚Äôre applying for. Consult any data science connections on what salaries they‚Äôve earned over their career and compare any roles you‚Äôre interested in with similar ones that do have a salary If you‚Äôre designing a data science course: include salary negotiation training. Inform learners of accurate, regularly updated salary ranges in the ‚Ä¶","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607472000,"objectID":"a221ae397c0eea88c07fd1b28a36fa8d","permalink":"/publication/what_do_uk_employers_want_dec_2020/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/publication/what_do_uk_employers_want_dec_2020/","section":"publication","summary":"Extracting useful insights about the UK data science job market utilizing web-scraped data, NLP techniques and unsupervised learning in the form of topic modelling.","tags":["NLProc","Web-scraping","Unsupervised Learning","Data-viz","Statistics"],"title":"What do UK employers want from a data scientist?","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart. ","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"bf1eb249db79f10ace7d22321494165a","permalink":"/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Ioana Fiona Preoteasa"],"categories":null,"content":"TL;DR ‚Äî Latent Dirichlet Allocation (LDA, sometimes LDirA/LDiA) is one of the most popular and interpretable generative models for finding topics in text data. I‚Äôve provided an example notebook based on web-scraped job description data. Although running LDA on a canonical dataset like 20Newsgroups would‚Äôve provided clearer topics , it‚Äôs important to witness how difficult topic identification can be ‚Äúin the wild‚Äù, and how you might not actually find clear topics ‚Äî with unsupervised learning, you are never guaranteed to find an answer!\nAcknowledgement: the greatest aid to my understanding was Louis Serrano‚Äôs two videos on LDA (2020). A lot of the intuition section is based on his explanation, and I would urge you to visit his video for a more thorough dissection. Fig 1.0 ‚Äî the LDA ‚Äúmachine‚Äù producing documents\nContents: Intuition\nMaths\nImplementation and visualisation\nIntuition Let‚Äôs say that you have a collection of different news articles (your corpus of documents), and you suspect that there are several topics that come up frequently within said corpus ‚Äî your goal is to find out what they are! To get there you make a few key assumptions:\nThe distributional hypothesis: Words that appear together frequently are likely to be close in meaning; each topic is a mixture of different words (Fig 1.1); each document is a mixture of different topics (Fig 1.2). Fig 1.1 ‚Äî Topics as a mixture of words\nIn Fig 1.1 you‚Äôll notice that the topic ‚ÄúHealth \u0026amp; Medicine‚Äù has various words associated with it to varying degrees (‚Äúcancer‚Äù is more strongly associated than ‚Äúvascular‚Äù or ‚Äúexercise‚Äù). Note that different words can be associated with different topics, as with the word ‚Äúcardio‚Äù.\nFig 1.2 ‚Äî Document as a mixture of topics\nIn Fig 1.2 you‚Äôll see that a single document can pertain to multiple topics (as colour-coded on the left). Words like ‚Äúinjury‚Äù and ‚Äúrecovery‚Äù might also belong to multiple topics (hence why I‚Äôve coloured them in more than one colour).\nNow LDA is a generative model ‚Äî it tries to determine the underlying mechanism that generates the articles and the topics. Think of it as if there‚Äôs a machine with particular settings that spits out articles, but we can‚Äôt see the machine‚Äôs settings, only what it produces. LDA creates a set of machines with different settings and selects the one that gives the best-fitting results (Serrano, 2020). Once the best one is found, we take a look at its ‚Äúsettings‚Äù and we deduce the topics from that.\nSo what are these settings?\nFirst, we have something called the Dirichlet (pronounced like dee-reesh-lay) prior of the topics. This is a number that says how sparse or how mixed up our topics are. In L Serrano‚Äôs video (which I highly recommend!) he illustrates how visually you can think of this as a triangle (Fig 1.3) where the dots represent the documents and their position with respect to the corners (i.e. the topics) represents the how they‚Äôre related to each of the topics (2020). So a dot that is very close to the ‚ÄúSports‚Äù vertex will be almost entirely about sport.\nFig 1.3 ‚Äî Dirichlet distribution of topics\nIn the lefthand triangle the documents are fairly separated, most of them neatly tucked into their corners (this corresponds to a low Dirichlet prior, alpha\u0026lt;1); on the right they are in the middle and represent a more even mix of topics (a higher Dirichlet prior, alpha\u0026gt;1). Look at the document in Fig 1.2 and, given the mix of topics, have a think about where you think it would be placed in the triangle on the right (my answer is that it‚Äôd be the dot just above the one closest to the Sports corner).\nSecond, we have the Dirichlet prior of the terms (all the words in our vocabulary). This number (whose name is beta) has almost exactly the same function as alpha ‚Äî except that it determines how the topics are distributed amongst the terms.\nFig 1.4 Dirichlet distribution of terms; the numbers are proportional to how much each word is associated with each respective topic\nAs we said before, the topics are assumed to be mixtures (more precisely, distributions) of different terms. In Fig 1.4 ‚ÄúSports‚Äù is mostly drawn towards ‚Äúinjury‚Äù. ‚ÄúHealth\u0026amp;Medicine‚Äù is torn between ‚Äúcardio‚Äù and ‚Äúinjury‚Äù and has no association with the term ‚Äúpray‚Äù.\nBut wait, our vocabulary doesn‚Äôt consist of just 3 words! You‚Äôre right! We could have a vocabulary of 4 words (as shown in Fig 1.5)! Trouble is that visualising a typical vocabulary of N words (where N could be 10\u0026#39;000) would require a generalised version of the triangle shape, but in N ‚Äî 1 dimensions (the term for this is an n-1 simplex). This is where the visuals stop and we trust that the maths of higher dimensions will function as expected. This also applies to the topics ‚Äî very often we‚Äôll find ourselves with more than 3 topics.\nFig 1.5 ‚Äî which topic is the red one, based on the distribution of terms?\nAn important clarification: in LDA we start with values of alpha and beta as hyperparameters, but these numbers only tell us whether our dots (documents / ‚Ä¶","date":1601078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601078400,"objectID":"97de9558876ce696430e2d0d3cc09ec1","permalink":"/publication/lda_medium/","publishdate":"2020-09-26T00:00:00Z","relpermalink":"/publication/lda_medium/","section":"publication","summary":"I explain the intuition behind LDirA and go through a worked example of applying and visualizing it to a canonical NLP dataset.","tags":["NLProc","Unsupervised Learning"],"title":"Latent Dirichlet Allocation: Intuition, math, implementation and visualisation with pyLDAvis","type":"publication"},{"authors":["Ioana Fiona Preoteasa"],"categories":null,"content":"TL;DR ‚Äî Transformers are an exciting and (relatively) new part of Machine Learning (ML) but there are a lot of concepts that need to be broken down before you can understand them. This is the first post in a column I‚Äôm writing about them. Here we focus on how the basic self-attention mechanism works, which is the first layer of a Transformer model. Essentially for each input vector Self-Attention produces a vector that is the weighted sum over the vectors in its neighbourhood. The weights are determined by the relationship or connectedness between the words. This column is aimed at ML novices and enthusiasts who are curious about what goes on under the hood of Transformers.\nContents: Introduction Self-Attention ‚Äî the math References 1. Introduction Transformers are an ML architecture that have been used successfully in a wide variety of NLP tasks, especially sequence to sequence (seq2seq) ones such as machine translation and text generation. In seq2seq tasks, the goal is to take a set of inputs (e.g. words in English) and produce a desirable set of outputs (- the same words in German). Since their inception in 2017, they‚Äôve usurped the dominant architecture of their day (LSTMs) for seq2seq and have become almost ubiquitous in any news about NLP breakthroughs (for instance OpenAI‚Äôs GPT-2 even appeared in mainstream media!).\nFig 1.1 ‚Äî machine translation (EN ‚Üí DE)‚Å¥\nThis column is intended as a very gentle, gradual introduction to the math, code and concept behind Transformer architecture. There‚Äôs no better place to start with than the attention mechanism because:\nThe most basic transformers rely purely on attention mechanisms¬≥.\n2. Self-Attention ‚Äî the math We want an ML system to learn the important relationships between words, similar to the way a human being understands words in a sentence. In Fig 2.1 you and I both know that ‚ÄúThe‚Äù is referring to ‚Äúanimal‚Äù and thus should have a strong connection with that word. As the diagram‚Äôs colour coding shows, this system knows that there is some connection between ‚Äúanimal‚Äù, ‚Äúcross‚Äù,‚Äústreet‚Äù and ‚Äúthe‚Äù because they‚Äôre all related to ‚Äúanimal‚Äù, the subject of the sentence. This is achieved through Self-Attention.‚Å¥\nFig 2.1 ‚Äî which words does ‚ÄúThe‚Äù pay attention to?‚Å¥\nAt its most basic level, Self-Attention is a process by which one sequence of vectors x is encoded into another sequence of vectors z (Fig 2.2). Each of the original vectors is just a block of numbers that represents a word. Its corresponding z vector represents both the original word and its relationship with the other words around it.\nFig 2.2: sequence of input vectors x getting turned into another equally long sequence of vectors z\nVectors represent some sort of thing in a space, like the flow of water particles in an ocean or the effect of gravity at any point around the Earth. You can think of words as vectors in the total space of words. The direction of each word-vector means something. Similarities and differences between the vectors correspond to similarities and differences between the words themselves (I‚Äôve written about the subject before here).\nLet‚Äôs just start by looking at the first three vectors and only looking in particular at how the vector x2, our vector for ‚Äúcat‚Äù, gets turned into z2. All of these steps will be repeated for each of the input vectors.\nFirst, we multiply the vector in our spotlight, x2, with all the vectors in a sequence, including itself. We‚Äôre going to do a product of each vector and the transpose (the diagonally flipped version) of x2 (Fig 2.3). This is the same as doing a dot product and you can think of a dot product of two vectors as a measure of how similar they are.\nFig 2.3: transposed multiplication (superscript ‚ÄúT‚Äù = ‚Äútransposed‚Äù)\nThe dot product of two vectors is proportional to the cosine of the angle between them (Fig 2.4) ‚Äî so the more closely they align in direction, the larger the dot product. If they were pointing in the exact same direction then the angle A would be 0‚Å∞ and a cosine of 0‚Å∞ is equal to 1. If they were pointing in opposite directions (so that A = 180‚Å∞) then the cosine would be -1.\nFig 2.4 ‚Äî dot product of two vectors\nAs an aside, note that the operation we use to get this product between vectors is a hyperparameter we can choose. The dot product is just the simplest option we have and the one that‚Äôs used in Attention Is All You Need¬≥ (AIAYN).\nIf you want an additional intuitive perspective on this, Bloem‚Äôs¬π post discusses how self-attention is analogous to the way recommender systems determine the similarity of movies or users.\nSo we put one word under the spotlight at a time and determine its output from its neighbourhood of words. Here we‚Äôre only looking at the words before and after but we could choose to widen that window in the future.\nFig 2.5 ‚Äî raw weights for each j-th vector\nIf the spotlit word is ‚Äúcat‚Äù, the sequence of words we‚Äôre going over is ‚Äúthe‚Äù, ‚Äúcat‚Äù, ‚Äúsat‚Äù. We‚Äôre asking how much attention the word ‚Äúcat‚Äù should pay to ‚Äúthe‚Äù, ‚Ä¶","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"61848222bdf68538a25bbe58706a7fb1","permalink":"/publication/self_attention_1/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/self_attention_1/","section":"publication","summary":"Breaking down the first part of transformer language models - the self-attention mechanism.","tags":["NLProc","Deep Learning"],"title":"1 | Basics of Self-Attention","type":"publication"},{"authors":null,"categories":null,"content":"Sentiment Analysis in Pytorch Training and deploying a Sentiment Analysis model in Pytorch using AWS Sagemaker. The data used was IMDB‚Äôs Movie review dataset (https://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 instances of binary labelled movie reviews.\nHow do we take a machine learning model and put it out there to be used? How do we deploy an inference model out there in the real world, to interact with a user? Amazon Web Services platform allows us to train, host and deploy a variety of models that we can use to gain insight from user data. For this instance, suppose we are looking to gauge opinions on a recent product, a film, a game, a trailer, etc.. Users might express their views via some form of text data (e.g. Tweets, reviews, Facebook posts, reddit comments). A deployed sentiment analysis model would be used to take in that data and infer the polarity of the data (positive or negative) and return to us, the number of positive and negative reviews respectively. This would be helpful to inform future product-related or marketing decisions regarding whatever product we‚Äôre putting out there.\nMy foci for this project were a) developing my understanding of successful model deployment on AWS Sagemaker and b) practicing writing a neural network model in pytorch and the appropriate data preprocessing functions for it. Given that this is a widely-used, canonical dataset, model performance was less of a priority. To that end I‚Äôm including below a flow-chart of the architecture (Figure 1) - to follow the logic of what happens to a user‚Äôs inputted review, start from the top left (‚ÄúUser‚Äôs review‚Äù) and follow the arrows to the right. I‚Äôve included the training phase in the same diagram as deployment but it should be noted that training - shown through the righthand part of the diagram (labelled training data fed into the Sagemaker model and then stored in S3 buckets) - happens prior to deployment.\nFig 1 - Flow-chart showing how a Sagemaker notebook instance containing a Pytorch model interacts with an API, Lambda function and webapp\nModel performance and issues are supervised through AWS‚Äô CloudWatch platform, which provides a chronological log of activity from the model endpoint.\nA basic webapp, API and Lambda function were written for this project as well but are non-functioning as having these hosted on Sagemaker incurs significant costs. If you‚Äôre interested in replicating the project, the code here was tested while the S3 buckets and notebooks were still fully functioning so everything should run if cloned to a Sagemaker notebook instance perfectly, asuming all files are copied over. If you‚Äôre curious to see what the final, user-facing product would look like, there are some examples in Figures 2 and 3:\nFig 2 - quite pleased that the model got past the sneakily positive adjectives.\nFig 3 - also pleased that this model seems to have taste\nNavigation:\nSageMaker Project.ipynb - main sagemaker notebook; contains more detailed and thorrough notes and comments on how the model is built and how the various components of AWS interact with each other website - directory containing webapp html code serve - directory with necessary deployment .py files: model.py - model training functions predict.py - model deployment and inference functions utils.py - preprocessing functions for text data train - directory with additional model training files: requirements.txt - files needed by AWS for model to run dict_data - directory containing word_dict.pkl - dictionary of unique integer to word key-value pairs A significant amount of the material for this project was learned through Udacity‚Äôs Machine Learning Engineer Nanodegree. I am grateful for their content and resources and recommend the course.\nReferences: Data - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011). Pytorch - https://pytorch.org/ If you‚Äôre interested in learning how to use Pytorch, they provide a variety of great tutorials at https://pytorch.org/tutorials/ Udacity - https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t\n","date":1591228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591228800,"objectID":"d5ab353891ea0538dc3de8c606b5a832","permalink":"/project/sentiment-analysis-modelling-with-pytorch/","publishdate":"2020-06-04T00:00:00Z","relpermalink":"/project/sentiment-analysis-modelling-with-pytorch/","section":"project","summary":"Built a sentiment classifier using Pytorch and deployed using AWS Lambda and Sagemaker.","tags":["Deep Learning","NLProc"],"title":"Sentiment Analysis with Pytorch","type":"project"},{"authors":["Ioana Fiona Preoteasa"],"categories":null,"content":"=========================================================\nHow do we extract themes and topic from text using unsupervised learning TL;DR ‚Äî Text data suffers heavily from high-dimensionality. Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction techniques that follows the same method as Singular Value Decomposition. LSA ultimately reformulates text data in terms of r latent (i.e. hidden) features, where r is less than m, the number of terms in the data. I‚Äôll explain the conceptual and mathematical intuition and run a basic implementation in Scikit-Learn using the 20 newsgroups dataset.\nLanguage is more than the collection of words in front of you. When you read a text your mind conjures up images and notions. When you read many texts, themes begin to emerge, even if they‚Äôre never stated explicitly. Our innate ability to understand and process language defies an algorithmic expression (for the moment). LSA is one of the most popular Natural Language Processing (NLP) techniques for trying to determine themes within text mathematically. LSA is an unsupervised learning technique that rests on two pillars:\nThe distributional hypothesis, which states that words with similar meanings appear frequently together. This is best summarised by JR Firth‚Äôs quote ‚ÄúYou shall know a word by the company it keeps‚Äù [1, p106] Singular Value Decomposition (SVD ‚Äî Figure 1) a mathematical technique that we‚Äôll be looking at in greater depth. Note that LSA is an unsupervised learning technique ‚Äî there is no ground truth. The latent concepts might or might not be there! In the dataset we‚Äôll use later we know there are 20 news categories and we can perform classification on them, but that‚Äôs only for illustrative purposes. It‚Äôll often be the case that we‚Äôll use LSA on unstructured, unlabelled data.\nLike all Machine Learning concepts, LSA can be broken down into 3 parts: the intuition, the maths and the code. Feel free to use the links in Contents to skip to the part most relevant to you. The full code is available in this Github repo.\nA note on terminology: generally when decomposition of this kind is done on text data, the terms SVD and LSA (or LSI) are used interchangeably. From now on I‚Äôll be using LSA, for simplicity‚Äôs sake.\nThis article assumes some understanding of basic NLP preprocessing and of word vectorisation (specifically tf-idf vectorisation).\nContents: Intuition: explanation with political news topics The Math: SVD as a weighted, ordered sum of matrices or as a set of 3 linear transformations The code implementation: in Python3 with Scikit-Learn and 20Newsgroups data References 1. Intuition (return to Contents)\nIn simple terms: LSA takes meaningful text documents and recreates them in n different parts where each part expresses a different way of looking at meaning in the text. If you imagine the text data as a an idea, there would be n different ways of looking at that idea, or n different ways of conceptualising the whole text. LSA reduces our table of data to a table of latent (hidden_)_ concepts.\nFigure 1: formula and matrix dimensions for SVD\nSuppose that we have some table of data, in this case text data, where each row is one document, and each column represents a term (which can be a word or a group of words, like ‚Äúbaker‚Äôs dozen‚Äù or ‚ÄúDowning Street‚Äù). This is the standard way to represent text data (in a document-term matrix, as shown in Figure 2). The numbers in the table reflect how important that word is in the document. If the number is zero then that word simply doesn‚Äôt appear in that document.\nFigure 2: Document Term matrix, after applying some sort of vectorisation, in our case TF-IDF (but Bag of Words would also do)\nDifferent documents will be about different topics. Let‚Äôs say all the documents are politics articles and there are 3 topics: foreign policy (F.P.), elections and reform.\nFigure 3: Document-Topic matrix (or Document- Latent-Concept if you prefer)\nLet‚Äôs say that there are articles strongly belonging to each category, some that are in two and some that belong to all 3 categories. We could plot a table where each row is a different document (a news article) and each column is a different topic. In the cells we would have a different numbers that indicated how strongly that document belonged to the particular topic (see Figure 3).\nNow if we shift our attention conceptually to the topics themselves, we should ask ourselves the following question: do we expect certain words to turn up more often in either of these topics?\nIf we‚Äôre looking at foreign policy, we might see terms like ‚ÄúMiddle East‚Äù, ‚ÄúEU‚Äù, ‚Äúembassies‚Äù. For elections it might be ‚Äúballot‚Äù, ‚Äúcandidates‚Äù, ‚Äúparty‚Äù; and for reform we might see ‚Äúbill‚Äù, ‚Äúamendment‚Äù or ‚Äúcorruption‚Äù. So, if we plotted these topics and these terms in a different table, where the rows are the terms, we would see scores plotted for each term according to which topic it most strongly belonged. Naturally there will be terms that feature in all three ‚Ä¶","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586476800,"objectID":"28e22839922e35dbaeca7ea8fd4fddfc","permalink":"/publication/lsa_medium/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/publication/lsa_medium/","section":"publication","summary":"I explain the intuition behind Latent Semantic Analysis (LSA) and go through a worked example of applying and visualizing it to a canonical NLP dataset.","tags":["NLProc","Unsupervised Learning"],"title":"Latent Semantic Analysis: intuition, math, implementation","type":"publication"},{"authors":["Ioana Fiona Preoteasa"],"categories":null,"content":"====================================\n1. Why should we be interested? Often in Data Science and Machine Learning we are constrained by a lack of adequate labelled data. We have the tried-and-tested Supervised Learning (SL) algorithms, we have an idea of what type of task and output we would like to see, we can even imagine that glorious insight we‚Äôd gain from applying our Neural Nets and Decision Trees, BUT we just don‚Äôt have the (labelled) data.\nGenerating labelled data sets is time-consuming, expensive and ludicrously tedious. Imagine having to go through several hundred thousand photos and tag them with the number of pine trees they contain. You‚Äôd get bored very quickly. Now imagine if you had to read through hundreds of thousands of lines of text, labelling them with whether the sentence is in active or passive voice. On top of being bored, you‚Äôll get sloppy at some point, so an additional labeller will probably be needed. The problem becomes worse for complex data points, such as long legal documents or health records. For any organisation trying to leverage its data, the cost of this (in terms of time, money and, frankly, morale!) will quickly ramp up! So instead of using SL, we can turn to a form of Semi-Supervised Learning, specifically, Active Machine Learning. 2. What is Active Machine Learning? The purpose of Active ML is to supply the smallest necessary amount of labelled data to produce a robust learner while minimising human intervention. The human labelling is restricted to those cases where it has the maximum usefulness. Now some questions arise immediately from that statement:\nHow do you determine the ‚Äúsmallest necessary amount‚Äù? Which data should go into that amount? What are we defining as ‚Äúusefulness‚Äù? I‚Äôll delve deeper into those issues in section 3. Figure 1 shows the general steps in active ML. The green numbers represent our steps detailed below:\nFig 1: general architecture of active machine¬†learning\nThe human expert only labels a subset of that data (say 10%; the individual steps are shown in green). This cuts down massively on labelling time. Let‚Äôs refer to this initial labelled data as the seed data.\nWe train our machine learner (which could be ANY type of algorithm suited to our task, be it regressor or classifier) JUST on the seed data.\nThe now trained learner generates predictions on the rest of the data set and provides a value of how confident it is for each of its predictions.\nThe learner returns the predictions with the lowest confidence ratings to the human domain expert (kind of like a student going to a teacher with their homework saying ‚ÄúI wasn‚Äôt really sure how to do these‚Ä¶‚Äù).\nThe human expert only labels these low-confidence data points.\nFinally the additionally labelled data is fed into the labelled data set and, along with the seed data, is used to retrain our machine learner.\nA lot of the problems arise in Step 1, where we select our seed data. Before addressing that, let‚Äôs go through an example.\nFig 2: a boundary case; the learner hasn‚Äôt quite figured out the features that make a ‚Äú4‚Äù a¬†‚Äú4‚Äù.\nImagine you‚Äôre training a classifier on the MNIST handwritten digits dataset (a collection of pixelated images of single digit numbers from 0 to 9). Suppose that we lost all the labels for this data. We could just the then you would train your learner on a subsection of the data that was labelled. It is useful to think of the low-confidence data points as being highly discriminant‚Ää-‚Ääthey are very good boundary cases that test your learner‚Äôs comprehension of the data. Then, when our learner makes predictions on the rest of the data, it will return us the predictions with the lowest confidence, as shown in\nFigure 2. A classifier might misidentify the number ‚Äú4‚Äù as a ‚Äú1‚Äù, but we can see why it would do so.\nFigure 3: I went with ‚Äú7‚Äù but honestly your guess is as good as¬†mine\nIn Figure 3 you can see an example of what this would look like in a Notebook (I used the standard example code for modAL, an Active Machine Learning library built to be compatible with Scikit-Learn).\n3. What are the limitations? How do you determine the ‚Äúsmallest necessary amount‚Äù? Which data should go into that amount? What are we defining as ‚Äúusefulness‚Äù?\nThis questions is domain- and data-specific. If you apply both standard supervised learning and AML to a dataset, as you increase the amount of initial labelled data supplied, your accuracy will increase in both cases. However, if the assumptions of AML are robust, then the accuracy curve will be steeper for the active learning process than for the standard supervised learning technique. In this case you should determine the relative costs of labelling more data and seeing what the uppermost limit on data labelling would be. There is no definitive answer (yet!) to this question, so empirically trying out the method on any previously labelled data sets to determine a cost-optimal threshold would be a good way to start. This is an area of active research: ‚Ä¶","date":1577923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577923200,"objectID":"2fc701a403b5f215a0000d5bc0051247","permalink":"/publication/active_ml/","publishdate":"2020-01-02T00:00:00Z","relpermalink":"/publication/active_ml/","section":"publication","summary":"I explain the process, motivation and potential behind Active Machine Learning, a form of running data labelling and model training in parallel to deal with scarcity of labelled data.","tags":["Machine learning"],"title":"Gently guiding the (machine) learner","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Exploring the UK Data Science job market through Indeed job posts I scraped 1082 job posts from Indeed.co.uk, searching for 3 different types of data science roles via the title, and then analysed the data. Please bear in mind that all the data was off one site only (I hope to be able to get job data off LinkedIn/Glassdoor at some point in the future) in late Nov 2020.\nBear in mind that this is an exploratory project - findings should be taken with a pinch of salt, and really the results need to be corroborated with a ~~replication of the project in 2021 or with other research. ~~ Update 01/06/2022: The scraping code has been updated again and tested on a small sample (n=10) of jobs with the search term title:(data,scientist).\nLocation is now taken directly from the search results page, which seems to be returning more locations than the alternative as it is a more easily identifiable element ‚ÄúRetrieve‚Äù has been removed from the GUI options. This was originally intended for the purpose of retrieving historical data from an SQLite db, but I‚Äôve mothballed that idea now since it didn‚Äôt give enough clear benefits. Findings Most Data Science jobs do not advertise their salary openly (only 41.5% do) The median Data Science salary is about ¬£45k; the mean is about ¬£51k The majority of roles are based in London The median Data Analyst role is under ¬£40k, whereas the median Data Scientist and Machine Learning jobs are above ¬£60k. Roles with ‚ÄúScientist‚Äù and/or ‚ÄúMachine learning‚Äù earn ¬£20k more (on average) than roles with ‚ÄúAnalyst‚Äù in the title. London and Cambridge have the highest average salaries; the former has the widest spread of salaries If you‚Äôre a median-salary London Data Analyst, you‚Äôre getting paid as well as the median salary for all Data Science roles outside the company. Just over 14% of overall roles are looking for a ‚ÄúSenior‚Äù hire. Less than 2% explicitly advertise for a ‚ÄòJunior‚Äô hire. The most popular languages / skills (based on our search) were: Python, SQL, R, AWS and Azure (in decreasing order of mentions). Using feature engineering and 3 different iterations of models, I was not able to predict salary from job description and title data. This indicates that either more data is needed (likely since there only 274 data points in the training data) or that it‚Äôs not possible to reliably predict the salary from such data. There was insufficient data to determine if years of experience required / requested correlated with annual salary. After using topic modelling, a few topics stood out - most noticeably the Academic_\u0026amp;_Research topic which seems to be the only one that has a moderate correlation with salary. Full method and implementation in notebook - Data_Scientist_UK_Q4_Job_market_analysis.ipynb\nFuture Steps Improve webscraping process to pick up ‚ÄúRemote work‚Äù tags that may be stored sepately on the job website to the main text Collect more data in 2021 and reiterate project to test current findings Using feature engineering and regex, for future batches, determine subgroup of jobs that are research-focussed and test again if those roles do correlate with salary. Repo navigation: index.ipynb - principal notebook; questions and project plan located there.\nscrape_indeed_gui.py - script for running Job Scraper Tool (built using pySimpleGui)\narchive/\ncleaning.ipynb - notebook to check the outputs of the scraping tool‚Äôs results\nuntitled.ipynb - nb used to load and check extraneous data (e.g. ONS salary information for sector)\nclean_data/ - folder preprocessed data\nraw_data/ folder including data as immediately outputed after webscraping stage\nLDA_vis_plot.html - interactive visualisation of Latent Dirichlet Allocation.\nfunctions.py - scripted functions and classes stored here, including webscraping tool\ntopic_mod.py - functions for topic modelling\nfig/ - noteable visualisations saved here\nReferences: SlashData Report - ‚ÄòState of the Developer Nation 19th Edition‚Äô - https://slashdata-website-cms.s3.amazonaws.com/sample_reports/y7fzAZ8e5XuKCL1Q.pdf Logistic Ordinal Regression - http://fa.bianp.net/blog/2013/logistic-ordinal-regression/ pyLDAvis Overview - https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb Key Assumptions to bear in mind: Data sourcing - Data was sourced purely from Indeed.co.uk over a limited time span. The individual time of scraping is recorded for each job post inside the raw_data file raw_data/SE_jobs_20_11_2020.csv. Further sampling over time will be needed to replicate or falsify findings. National averages - ONS data was filtered to retrieve data for 2019 as 2020 data is a. not fully available yet broken down by and b. 2020 survey data was affected by the Covid19 lockdown and the move to telephone polling. Although the 2019 mean salary is not a fair comparison for data retrieved in Q3 2020, it provides a rough benchmark against which we can compare our sample. Data mining - The salary extraction method is reliant on the pattern ‚Ä¶","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c690cf10fe45461d5925a3c554b77b87","permalink":"/project/job-market-exploration/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/job-market-exploration/","section":"project","summary":"Webscraped, cleaned and analysed job posts for different types of data science specialists.","tags":["Web-scraping","NLProc","Unsupervised-Learning"],"title":"Analysing scraped job data","type":"project"}]