<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Academic</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Academic</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Academic</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing and quantifying topics on Twitter</title>
      <link>/project/twitter_sentiment_tracking/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      <guid>/project/twitter_sentiment_tracking/</guid>
      <description>&lt;h2 id=&#34;using-osint-tools-and-transformers-to-extract-topics-and-sentiment&#34;&gt;Using OSINT tools and Transformers to extract topics and sentiment&lt;/h2&gt;
&lt;p&gt;Using the Blattodea tool that I helped develop during a hackathon, I retrieved the most recent tweets from Elon Musk. I then used one of &lt;a href=&#34;https://huggingface.co/models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HuggingFace&amp;rsquo;s&lt;/a&gt; pre-trained sentiment classification models and &lt;a href=&#34;https://maartengr.github.io/BERTopic/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERTopic&lt;/a&gt; to extract and visualize key themes.
I have also developed an RShiny dashboard for this project to hone my interactive visualization skills.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dashboard screenshot&#34; srcset=&#34;
               /project/twitter_sentiment_tracking/fig/dash_screenshot_hu7c0338f64b56aad8b96fcf4e43899430_140612_66a23c97d2bd90a9bd9aa98aedfa22f3.webp 400w,
               /project/twitter_sentiment_tracking/fig/dash_screenshot_hu7c0338f64b56aad8b96fcf4e43899430_140612_85f1f8c082af3f4339824fb7133c71d7.webp 760w,
               /project/twitter_sentiment_tracking/fig/dash_screenshot_hu7c0338f64b56aad8b96fcf4e43899430_140612_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;/project/twitter_sentiment_tracking/fig/dash_screenshot_hu7c0338f64b56aad8b96fcf4e43899430_140612_66a23c97d2bd90a9bd9aa98aedfa22f3.webp&#34;
               width=&#34;760&#34;
               height=&#34;536&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;note-on-utility-and-future-projects&#34;&gt;Note on utility and future projects&lt;/h2&gt;
&lt;p&gt;BERTopic can accommodate &lt;a href=&#34;https://maartengr.github.io/BERTopic/getting_started/online/online.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;online topic modelling&amp;rdquo;&lt;/a&gt; (i.e. incrementally adjusts topics with new data), and the results in this project have shown the model to be qualitatively coherent (lacking a labelled dataset, I am unable to compute the exact accuracy of clustering/classification). It would not be difficult to expand this work to a more regular, (semi-)automated pipeline to &lt;em&gt;monitor social media content&lt;/em&gt; around a particular hashtag/person/theme and to extract insight or detect sudden changes. Suppose you were a news organisation looking to gauge interest in a particular recent event. Although social media isn&amp;rsquo;t representative of general discourse around any given topic, taking data from Twitter, passing it through BERTopic and then setting the pipeline to regularly update the data and topics, would give you the data to assess at least some of the discussion around a theme or event.&lt;/p&gt;
&lt;h2 id=&#34;results-and-thoughts&#34;&gt;Results and thoughts&lt;/h2&gt;
&lt;p&gt;(Results are analysed and visualized at greater length inside the repo&amp;rsquo;s index.Rmarkdown notebook. For cleaning, EDA and modelling code, please see the Jupyter notebooks in the repo, explained in the filing system below)&lt;/p&gt;
&lt;h3 id=&#34;overall-usage&#34;&gt;Overall usage&lt;/h3&gt;
&lt;p&gt;I have been able to extract clear and definite topics from the collected data, and the pattern of activity around key themes has been what I expected it to be. For example, Musk&amp;rsquo;s opining on ending the war in Ukraine generated a larger amount of responses across the board than his other tweets. Through this project I&amp;rsquo;ve found that BERTopic has been extremely useful and capable of extracting information from unstructured text data, and I plan on using it in future projects. Moreover, the model was able to group topic clusters at a greater level of precision and accuracy than I had honestly expected. In the dendrogram included below I show how some of the topics (the most interesting ones) have been grouped by the model and what the model tells us vs what we can infer. For an NLP project is arguably even more important than in most data science projects to combine contextual knowledge and data viz with the results: language is far more ambiguous and mysterious than numbers.
Note that the tweets analysed here are all from 1st August 2022 onwards.&lt;/p&gt;
&lt;h3 id=&#34;clustering-topics&#34;&gt;Clustering Topics&lt;/h3&gt;
&lt;p&gt;Given the time-range for our data, it should be no surprise that tweets about the war have formed their own distinct group of clusters. It&amp;rsquo;s easy to see how topics 35, 36, 1 and 10 are linked. For interpreting this graph, recall that the most important number to count is the number of steps to take along the tree for two topics/leafs to connect.
(Note that these are graphs generated with the help of BERTopic and there&amp;rsquo;s far less you can do in terms of graph customization).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Hierarchy of our topics of interest&#34; srcset=&#34;
               /project/twitter_sentiment_tracking/fig/hierarchical_select_top_hu70c2815d607fc6af5045f54f172892cf_68184_cdc00f0f301053cdfb0b5bf7aef0b408.webp 400w,
               /project/twitter_sentiment_tracking/fig/hierarchical_select_top_hu70c2815d607fc6af5045f54f172892cf_68184_68c54f40c6fb247aa0c0d1791a7c91ed.webp 760w,
               /project/twitter_sentiment_tracking/fig/hierarchical_select_top_hu70c2815d607fc6af5045f54f172892cf_68184_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;/project/twitter_sentiment_tracking/fig/hierarchical_select_top_hu70c2815d607fc6af5045f54f172892cf_68184_cdc00f0f301053cdfb0b5bf7aef0b408.webp&#34;
               width=&#34;760&#34;
               height=&#34;357&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Topics 29 and 9 form an understandable cluster together as there were a significant number of tweets focused on Starlink&amp;rsquo;s activity in Ukraine (and its commercial activity more generally), both by Musk and his followers.
The grouping of clusters 3 and 2 is more interesting: closer inspection of 2 revealed that it includes some tweets related to Twitter bots (something Musk has made a point of discussing openly recently), which would link it sensibly to topic 3; however some of the tweets were also referring to Tesla &amp;lsquo;bots&amp;rsquo; (i.e. Tesla&amp;rsquo;s robotics research and department). If we&amp;rsquo;d had a set of topic labels for each of these, it&amp;rsquo;s very likely that BERTopic would misclassify the tweets in this particular topic. Topic two tweets range from :&lt;/p&gt;
&lt;p&gt;| &amp;ldquo;Tesla Bot is the future of robotics 🤯 &amp;quot;&lt;/p&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;p&gt;| &amp;ldquo;If Twitter put as much effort into botspam removal as they do into subpoenas we wouldnt have this problem in the first place&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Human language users like us can tell that these are two distinct themes. Yet I&amp;rsquo;d say that this amalgamation of is quite an understandable mistake, given the amount of words that appear across topic 2 that make it strongly related to topic 3 and others. It is possible that if we had more documents, BERTopic would&amp;rsquo;ve split topic 2 into another leaf.&lt;/p&gt;
&lt;p&gt;Towards the bottom we can also see three topics that, although not as closely aligned as the war-themed ones, still cluster together. These are all connected by the theme of rocketry, SpaceX and all of Musk&amp;rsquo;s space-related endeavours. It&amp;rsquo;s encouraging to see that the model was able to place these closely together. Note the next leaf that joins this particular sub-branch, topic 55, related to engines. Now, when I originally looked at this topic, I figured that the algorithm had misgrouped again and that discussions of engines must be focussed on cars. However, I was wrong and the algo was right - inspection of the tweets with this as their main topic (currently topic nr 48 in our merged data) revealed that my intuition was wrong:&lt;/p&gt;
&lt;p&gt;| &amp;ldquo;Hi Elon  😀 According to  380 tankers delivered to Starbase So I would be really happy to know Will B7 performs a long duration full stack 33 engines static fire Thanks🙏😀&amp;rdquo;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;| &amp;ldquo;How certain are you on the final number of engines for Superheavy Booster&amp;rdquo;&lt;/p&gt;
&lt;p&gt;and, a slight outlier in some sense:&lt;/p&gt;
&lt;p&gt;| &amp;ldquo;there will be a day when there are literally 69 boosters 🤓&amp;rdquo;&lt;/p&gt;
&lt;p&gt;(I do enjoy points of childish levity in a dataset.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyway&lt;/em&gt;, this is another great example of BERTopic&amp;rsquo;s strengths as a model.&lt;/p&gt;
&lt;h3 id=&#34;twitter-stats-over-time&#34;&gt;Twitter stats over time&lt;/h3&gt;
&lt;p&gt;If we take a look at the by-day total likes, re-tweets and responses, it&amp;rsquo;s clear that 3rd Oct 2022, when Musk posted his poll on the war, was a bit of watershed in terms of twitter stats (note that the ones in the plot are scaled down for comparison&amp;rsquo;s sake- he didn&amp;rsquo;t just get under 5 responses!). This particular day certainly generated the most conversation (if we take number of responses as a proxy) and in generally most of his tweet stats increased somewhat in the period after. Apart from the one-day spike, it can hardly be said that the furore many felt as a result of the 3rd Oct twitter poll has manifested at the tweet meta-data level.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Stats for Musk&amp;amp;rsquo;s tweets over time&#34; srcset=&#34;
               /project/twitter_sentiment_tracking/fig/ggplot_musk_all_total_tweets_feats_hu3ba6f4586ae26ee2ba8466bf160ced7b_282186_df48baed456d29cace13c80db6489c6b.webp 400w,
               /project/twitter_sentiment_tracking/fig/ggplot_musk_all_total_tweets_feats_hu3ba6f4586ae26ee2ba8466bf160ced7b_282186_e05b136ea41cef6c84108e220707709a.webp 760w,
               /project/twitter_sentiment_tracking/fig/ggplot_musk_all_total_tweets_feats_hu3ba6f4586ae26ee2ba8466bf160ced7b_282186_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;/project/twitter_sentiment_tracking/fig/ggplot_musk_all_total_tweets_feats_hu3ba6f4586ae26ee2ba8466bf160ced7b_282186_df48baed456d29cace13c80db6489c6b.webp&#34;
               width=&#34;760&#34;
               height=&#34;543&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;If we compare &lt;em&gt;average&lt;/em&gt; values before and after the poll went out (shown below with the box and jitter plot- each point represents one tweet), then we see barely any changes at all. The distribution of retweets and responses seems to be somewhat more skewed, but looking at it it&amp;rsquo;s not even worth doing a statistical test.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;EDA_twitter_sentiment_tracking/boxplot_before_and_after.png&#34; alt=&#34;Stats for Musk&amp;amp;rsquo;s tweets before and after&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;How about when we combine tweet stats with the topics, just for Elon Musk&amp;rsquo;s tweets? Well, the heatmap below shows that there isn&amp;rsquo;t much correlation between the probability scores of each of our main topics of interest and any of the three tweet features:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;EDA_twitter_sentiment_tracking/corr_heatmap.png&#34; alt=&#34;Correlation heatmap of main topics and tweet stats&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are two correlations worth testing for here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nr of likes and tweets on SpaceX&lt;/li&gt;
&lt;li&gt;nr of responses and tweets on Russia-Ukraine war&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s check them for statistical significance.&lt;/p&gt;
&lt;p&gt;| H0: there is no statistically significant correlation between our determined* cluster of Elon Musk&amp;rsquo;s tweets on the Russia-Ukraine war and the number of responses to his tweets.&lt;/p&gt;
&lt;p&gt;| H1: there is a statistically significant correlation between our determined* Elon Musk&amp;rsquo;s tweets on the Russia-Ukraine war and the number of responses to his tweets.&lt;/p&gt;
&lt;p&gt;Setting alpha = 0.05/(number of tests) = 0.025&lt;/p&gt;
&lt;p&gt;| H0: there is no statistically significant correlation between our determined* cluster of Elon Musk&amp;rsquo;s tweets on SpaceX and the number of likes to these tweets.&lt;/p&gt;
&lt;p&gt;| H1: there is a statistically significant correlation between our determined* Elon Musk&amp;rsquo;s tweets on SpaceX and the number of likes to these tweets.&lt;/p&gt;
&lt;p&gt;Setting alpha = 0.05/(number of tests) = 0.025&lt;/p&gt;
&lt;p&gt;*I&amp;rsquo;m referring to them as determined by us because these tweets were categorised via a semi-supervised method, and they are not a gold-standard dataset that has been hand-labelled.&lt;/p&gt;
&lt;h3 id=&#34;sentiment-and-topics&#34;&gt;Sentiment and Topics&lt;/h3&gt;
&lt;p&gt;I was interested to see how different topics within this data in terms of the sentiment. To that end, I passed the tweets through a pre-trained sentiment classifier to infer whether they were positive or negative. Below I show how this varies over time for the Tweets directly mentioning Elon Musk:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Sentiment around Musk by topic&#34; srcset=&#34;
               /project/twitter_sentiment_tracking/fig/ggplot_pct_tweets_by_topic_and_sentiment_hub766bd4ccc75a0dda2f3385562008020_455449_5fd6e367845b11c845a000c67d512f55.webp 400w,
               /project/twitter_sentiment_tracking/fig/ggplot_pct_tweets_by_topic_and_sentiment_hub766bd4ccc75a0dda2f3385562008020_455449_90c48b09a35b4ca2367466a217ef5f56.webp 760w,
               /project/twitter_sentiment_tracking/fig/ggplot_pct_tweets_by_topic_and_sentiment_hub766bd4ccc75a0dda2f3385562008020_455449_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;/project/twitter_sentiment_tracking/fig/ggplot_pct_tweets_by_topic_and_sentiment_hub766bd4ccc75a0dda2f3385562008020_455449_5fd6e367845b11c845a000c67d512f55.webp&#34;
               width=&#34;760&#34;
               height=&#34;543&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important to note that the amount of tweets is hard to include in the graph and the y-axis reports on percentages by day. Prior to early autumn &amp;lsquo;22, the number of tweets mentioning Elon and concerned with the war was significantly lower. Note that although there is a trend line included, it wherever the green and red lines run tangentially to each other near 100% is to be ignored: those are days with very few datapoints and where the % sentiment per day flips between fully positive and fully negative easily.
I&amp;rsquo;ve tried to represent the number of tweets in the opacity/transparency of the points, however, that is skewed by the fact the outlier of negative tweets Musk received the around the time he released his poll. Nevertheless, at least within this limited subset of his audience, there appears to have been a clear rise in the proportion of negative tweets, especially around the theme of the war. However, this batch is not representative of course: partially for the reason stated earlier about how the data was collect but also because these topics do not cover the full range of sentiment around Musk. Consider the fact that there could easily have been someone tweeting &amp;ldquo;@elonmusk, please stay out of global affairs&amp;rdquo; and this would likely not have been categoriesed as part of the war-theme topic cluster. Therefore, we are very likely undercounting. Realistically, here are only looking at the subset of tweets that were in some way in Elon&amp;rsquo;s extended network, that mentioned him and that mentioned terms directly relevant to the war, so that someone with no prior knowledge could have picked them up and said &amp;ldquo;yes, this Tweet has X opinion about Musk on this topic&amp;rdquo;. Lastly, there is the possibility of tweets that mention Musk but without actually being focussed on him: e.g. someone making a negative statement about the war in general and @ Musk in it for different reasons.&lt;/p&gt;
&lt;p&gt;Perhaps the opacity of the points should be a greater focus of attention: it would appear that attention flared up dramatically on Musk (for this segment of the data) and then died down again slowly, but maintained the tendency to be negative.&lt;/p&gt;
&lt;h3 id=&#34;bottom-line&#34;&gt;Bottom-line&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Musk&amp;rsquo;s tweets on the war did generate a significant response on Twitter, although this appears to have been short-lived&lt;/li&gt;
&lt;li&gt;These tweets form very clear clusters of topics and can be amalgamated with other, tangentially related topics.&lt;/li&gt;
&lt;li&gt;BERTopic is an awesome tool that I&amp;rsquo;m looking forward to using again!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;repo-filing-system&#34;&gt;Repo filing system:&lt;/h3&gt;
&lt;p&gt;Notebooks&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;index.md - principal file of findings and final results; most relevant notebook to most people
2a. Topic_modelling_with_BERT.ipynb - notebook details the journey of analysing the model results and extracting insights.
2b. Modelling_w_BERTopic_GColab.ipynb - Google Colab notebook where the BERTopic and sentiment models&amp;rsquo; results were generated.&lt;/li&gt;
&lt;li&gt;EDA.ipynb - rough exploration of the data; go here for more in-depth look at some of the data. Most of the visualizations therein were not used.&lt;/li&gt;
&lt;li&gt;data_cleaning.ipynb - notebook detailing the entire cleaning process. Primarily relies on py modules within functions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Folders&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data - folder containing raw, processed, clean and feature data and any additional summary data generated. Also contains inferred data (i.e. tweets and their predicted sentiment; tweets and their associated topics)&lt;/li&gt;
&lt;li&gt;models - the fitted BERTopic model files - &lt;strong&gt;NOTE&lt;/strong&gt; unfortunately, due to a bug in the way the models were saved, it is not possible to load them up locally on a lot of machines. However, it is possible to reproduce their creation and fitting on Google Colab, using the &lt;a href=&#34;https://github.com/Ioana-P/IoanaFio/blob/main/content/project/twitter_sentiment_tracking/Modelling_w_BERTopic_GColab.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt; inside this repo
*fig - all data viz (including interactive HTML files)
*functions&lt;/li&gt;
&lt;li&gt;archive - any additional files and subfolders will be here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;References:
@article{grootendorst2022bertopic,
title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
author={Grootendorst, Maarten},
journal={arXiv preprint arXiv:2203.05794},
year={2022}
}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to Wowchemy, the website builder for Hugo</title>
      <link>/post/getting-started/</link>
      <pubDate>Sun, 13 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/getting-started/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;
















&lt;figure  id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&#34;get-started&#34;&gt;Get Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;👉 &lt;a href=&#34;https://wowchemy.com/templates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Create a new site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;📚 &lt;a href=&#34;https://wowchemy.com/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Personalize your site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💬 &lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chat with the &lt;strong&gt;Wowchemy community&lt;/strong&gt;&lt;/a&gt; or &lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Hugo community&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;🐦 Twitter: &lt;a href=&#34;https://twitter.com/wowchemy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@wowchemy&lt;/a&gt; &lt;a href=&#34;https://twitter.com/GeorgeCushen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@GeorgeCushen&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23MadeWithWowchemy&amp;amp;src=typed_query&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#MadeWithWowchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;💡 &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request a &lt;strong&gt;feature&lt;/strong&gt; or report a &lt;strong&gt;bug&lt;/strong&gt; for &lt;em&gt;Wowchemy&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Wowchemy?&lt;/strong&gt; View the &lt;a href=&#34;https://wowchemy.com/docs/hugo-tutorials/update/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Update Tutorial&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/updates/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-wowchemys-future-httpswowchemycomplans&#34;&gt;&lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;❤️ Click here to become a sponsor and help support Wowchemy&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock &lt;a href=&#34;https://wowchemy.com/plans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;these&lt;/a&gt; awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/wowchemy/hugo-academic-cli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugo Academic CLI&lt;/a&gt;:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://wowchemy.com/user-stories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://wowchemy.com/docs/page-builder/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://wowchemy.com/docs/content/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://wowchemy.com/docs/import/jupyter/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://wowchemy.com/docs/install-locally/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://wowchemy.com/docs/customization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 34+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Wowchemy and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demo&lt;/a&gt; to see it in action! Day/night mode can also be disabled by the site admin in &lt;code&gt;params.toml&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/customization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Choose a stunning &lt;strong&gt;theme&lt;/strong&gt; and &lt;strong&gt;font&lt;/strong&gt;&lt;/a&gt; for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What do UK employers want from a data scientist?</title>
      <link>/publication/what_do_uk_employers_want_dec_2020/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/what_do_uk_employers_want_dec_2020/</guid>
      <description>&lt;p&gt;Picture a novice data scientist, either straight out of education or pivoting careers. They are determined to enter the field but have limited time and funds to spare in extra training and are daunted by the sheer variety of skills and experiences employers request. Naturally, they sign up to intense bootcamps and scramble to take online courses on everything from Azure to Excel.&lt;/p&gt;
&lt;p&gt;How can you be sure that focusing on a particular skill won’t prove to be a massive opportunity cost? How many years experience are they expecting? What should I expect to get paid, realistically? If you’re leafing through a data science syllabus, and you want to make sure your investment pays off then what should that syllabus include? That’s also a very pertinent question for people designing a data science course: how do we maximise the overlap between what recruiters want and what our course teaches?&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*Qn32iuMQeL32uS6Z3B5hBw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Every new or aspiring data scientist right now— Img 1.1 — Photo courtesy of &lt;a href=&#34;https://unsplash.com/photos/7iSEHWsxPLw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UnSplash&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;about-this-post&#34;&gt;&lt;strong&gt;About this post&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I’m reporting on findings from an exploration of job data on Indeed.co.uk, so this focusses on the UK job market. Although this post provides useful answers, there are still plenty of questions to ask and it’s unclear how representative the findings are of the total population of jobs. This is why future iterations of the project in Q1 2021 and later will try to replicate / falsify the findings. If you have suggestions for other things to search for within the data or any critiques of methods used, please leave a comment. All feedback is appreciated.&lt;/p&gt;
&lt;p&gt;If you just want the most important results, scroll to section &lt;strong&gt;II. Key insights&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you want to know how those results were generated in more detail, scroll to section &lt;strong&gt;III. Methodology&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If you want to be able to replicate the findings, read the assumptions or see the results in full (including more of the null results), see the &lt;a href=&#34;https://github.com/Ioana-P/DS_Indeed_Job_Market_analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;README in the project repo&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;and the&lt;/strong&gt; &lt;a href=&#34;https://github.com/Ioana-P/DS_Indeed_Job_Market_analysis/blob/master/Data_Scientist_UK_Q4_Job_market_analysis.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;main notebook&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;i-why-and-how&#34;&gt;I. Why and How&lt;/h1&gt;
&lt;p&gt;Harvard Business Review dubbed Data Scientist the &lt;a href=&#34;https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sexiest job of the century&lt;/a&gt;, and the rapid wave of data bootcamps and online courses over the past decade reflects the immense magnetism of the profession. Yet with so many people rushing into data science, it’s important to know the market and offer a &lt;em&gt;competitive resume&lt;/em&gt;. This post and its &lt;a href=&#34;https://github.com/Ioana-P/DS_Indeed_Job_Market_analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;corresponding project repository&lt;/a&gt; represent my initiative in adding to the trove of knowledge on the job market. This is an exploratory, single-researcher analysis, using data scraped only from Indeed, with a total sample of 1082 job descriptions and 382 annual salaries. Therefore you should take findings with a pinch of salt. The plan is to replicate the scraping and analysis during Q1 2021. I plan to broaden the range of analyses performed on the data as well as improve the quality and efficiency of the web-scraping tool. I searched for jobs with 3 different pairs of words in the title:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data, scientist (DS)&lt;/li&gt;
&lt;li&gt;machine, learning (ML)&lt;/li&gt;
&lt;li&gt;data, analyst (DA)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ve included all of those under the umbrella of “data science roles” because even though it’s common knowledge that those roles do very different things and require different proficiencies, they still all &lt;em&gt;do&lt;/em&gt; data science, and the lines between the categories is blurry.&lt;/p&gt;
&lt;h2 id=&#34;who-does-this-analysis-benefit-who-are-potential-stakeholders&#34;&gt;Who does this analysis benefit? Who are potential stakeholders?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Let’s imagine that I am an aspiring data scientist, recently starting out in the field. Regardless of my current qualifications, I want to know what employers want so I know what skills I need to go and &lt;strong&gt;acquire&lt;/strong&gt;, which of my skills I can best &lt;strong&gt;leverage,&lt;/strong&gt; and what I else I should learn over time to increase my potential salary.&lt;/li&gt;
&lt;li&gt;Let’s suppose that we are a data science course provider / bootcamp based in the UK. Our bottom line is &lt;strong&gt;getting our learners hired&lt;/strong&gt; in the data science world &lt;em&gt;and&lt;/em&gt;, additionally, trying to &lt;strong&gt;maximise the average salaries&lt;/strong&gt; our graduates get. To do so, we need to match our curriculum to what the market is asking for. What’s the right combination of skills, programming languages and expertise that we should be delivering? That’s a question we can answer by looking at employer needs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;ii-key-insights&#34;&gt;II. Key insights&lt;/h1&gt;
&lt;h2 id=&#34;1-3-out-of-5-data-science-roles-do-not-state-salary-openly-59&#34;&gt;1. 3 out of 5 data science roles do not state salary openly (~59%):&lt;/h2&gt;
&lt;p&gt;The discrepancy is consistent across the three categories, though wider for roles with “machine learning” (ML) in the title than those with “data analyst” (DA). This is particularly daunting for the lone, fresh data scientist entering the field, especially if they have no prior experience of negotiating pay in any field.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If you’re an aspiring data scientist:&lt;/strong&gt; use &lt;a href=&#34;https://www.glassdoor.co.uk/index.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glassdoor&lt;/a&gt; to research salaries at a company (where possible) and always bear in mind the average salary for that type of role you’re applying for. Consult any data science connections on what salaries they’ve earned over their career and compare any roles you’re interested in with similar ones that do have a salary&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If you’re designing a data science course: include salary negotiation training&lt;/strong&gt;. Inform learners of accurate, regularly updated salary ranges in the market.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-analyst-roles-are-most-numerous-but-least-well-paid&#34;&gt;&lt;strong&gt;2. Analyst roles are most numerous but least well paid:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Roles with “data scientist” (DS) and ML in the title are paid on average £25k per year more. That’s approximately £13 per hour more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why does this matter?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you’re aiming to maximise your salary, it’s important to know what parts of the field return greatest financial rewards over time. Having said that, consider that experience can be the greatest barrier to getting a job in data science (see Insight 4). &lt;strong&gt;If&lt;/strong&gt; an analyst role is easier to get into, but you still want to aim for the higher salaried positions, then why not build on it?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendations:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If&lt;/strong&gt; maximum salary is what you’re aiming for, then acquire skills that set you up for DS and ML roles (see Insight 3).&lt;/li&gt;
&lt;li&gt;Where possible use DA roles to build experience in the field — there is significant overlap between the skills required for DA and DS roles in particular (as shown in Insight 3).&lt;/li&gt;
&lt;li&gt;If you are more interested in analysing and reporting on data, then take comfort in the fact that there are &lt;strong&gt;more&lt;/strong&gt; jobs suitable to that goal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For course-providers&lt;/strong&gt;: inform learners of the full range of salaries available to them and make sure they have realistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-the-most-useful-skills-across-all-three-groups-are-pythonhttpswwwpythonorg-and-sqlhttpsenwikipediaorgwikisql&#34;&gt;&lt;strong&gt;3. The most useful skills across all three groups, are&lt;/strong&gt; &lt;a href=&#34;https://www.python.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Python&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;and&lt;/strong&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/SQL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;SQL&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For those just entering the field, that’s enough information to tell you where to start focusing your attention. Once you have a solid foundation in those two, here are your options:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*IR6XEm5D8i8tT0tzWGyiMQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.1 — the top 10 most mentioned skills / languages for each group&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-project.org/about.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;R programming&lt;/strong&gt;&lt;/a&gt; comes up in about 47% of DS jobs, and some jobs state that although they’d accept &lt;strong&gt;Python&lt;/strong&gt;, they’d &lt;em&gt;prefer&lt;/em&gt; someone who knows R. Moreover R offers a lot of statistical analysis tools that Python doesn’t have direct equivalents to, &lt;strong&gt;so if you’re going for stats-heavy roles&lt;/strong&gt;, R can give your resume an edge.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cloud platforms —&lt;/strong&gt; preferably train in &lt;a href=&#34;https://aws.amazon.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon Web Services&lt;/a&gt; (&lt;strong&gt;AWS&lt;/strong&gt;), but about 18% of all jobs (and &lt;strong&gt;&lt;em&gt;28%&lt;/em&gt; of ML roles&lt;/strong&gt;) mention &lt;em&gt;at least one&lt;/em&gt; of the 3 main cloud platforms. This is more useful if you’re heading into or training towards a DS or an ML Engineering role. The latter mention &lt;strong&gt;AWS&lt;/strong&gt; ~ 25% of the time. The exception is if you’re applying for DA roles, in which case &lt;a href=&#34;https://azure.microsoft.com/en-us/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Azure&lt;/strong&gt;&lt;/a&gt; is more popular than the others, but still features in only about 6% of those roles.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If you’re going more towards the analyst route&lt;/strong&gt; — you will likely not benefit anywhere near as much from learning &lt;a href=&#34;https://www.scala-lang.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scala&lt;/a&gt;, &lt;a href=&#34;https://www.java.com/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Java&lt;/a&gt; or &lt;a href=&#34;https://www.docker.com/?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=dockerhomepage&amp;amp;utm_content=nemea&amp;amp;utm_term=dockerhomepage&amp;amp;utm_budget=growth&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt;, but you &lt;em&gt;will&lt;/em&gt; if you’re inclined towards &lt;strong&gt;ML roles!&lt;/strong&gt; The first two are popular languages for developing and optimising machine learning models, so it’s expected that ML roles would feature them. Docker is used widely for product deployment, so it would be wise to invest time in it if you’re aiming for ML.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For bootcamps:&lt;/strong&gt; emphasise to learners which skills get them where and make some parts of the course optional / elective: e.g. schedule an Intro to AWS lesson for all the attendants, but those who want to pursue analytics can stop attending after the intro and get more ROI taking an Advanced SQL class.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Place less focus on deep learning:&lt;/strong&gt; as popular as neural networks have become over the past couple of decades, they did not feature prominently, even after repeated searches through the texts for many different variations. Deep learning primarily appears in a special niche of data science roles, more towards the ML side of jobs, but is &lt;strong&gt;not mandatory or useful&lt;/strong&gt; to the majority of data science roles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-you-might-need-experience-to-get-experience&#34;&gt;&lt;strong&gt;4. You might need experience to get experience:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1008/1*H5krNpINKuw75UNZKVdm4A.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.2 — when ranges were stated they were averaged (e.g. “2–3 years” became 2.5)&lt;/p&gt;
&lt;p&gt;Most jobs that could be found to explicitly state an experience threshold asked for 2 to 3 years experience. &lt;strong&gt;If&lt;/strong&gt; representative, this presents the largest barrier to entering a data science role. If you’re completely lacking in experience now, you might have around &lt;strong&gt;20 jobs open to your level&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After 2 years working in data science, that number should be &lt;strong&gt;3 times bigger&lt;/strong&gt;. Moreover, the expected salary will increase too, as there is a moderate correlation between the two.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;If you’re just starting out in the field:&lt;/strong&gt; building experience is crucial, so widen your expectations of the work you’ll be doing to achieve that. You may not have a dream role this year or next, but the available paths into that dream job open up significantly after only 2 years.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Play the long game:&lt;/strong&gt; be strategic and realistic about what job you’re applying for and make 1, 3 and 5 year plans that includes what skills you hope to have by each milestone and what salary you’d expect.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;If you’re still struggling to land a role&lt;/strong&gt; then build experience doing freelance data science projects, data hackathons or charitable data science work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For course providers:&lt;/strong&gt; make sure learners are aware of the experience barrier and also try to build connections in the job market to create mini-opportunities for grads. Employers might be skeptical of taking bootcamp grads for a position, but they might be more open to the idea of facilitating internships and single project partnerships.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-the-gap-between-the-capital-and-the-rest-of-the-country&#34;&gt;&lt;strong&gt;5. The gap between the Capital and the rest of the country:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;As you might expect, &lt;strong&gt;just over half of the jobs are in London&lt;/strong&gt;. They also pay on average 50% more than in the rest of the country, which is to be expected given the much higher cost of living.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*aTSpt_0kgjbM32S4BYIDNQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.3&lt;/p&gt;
&lt;p&gt;The median data analyst in London would expect to earn as much as the overall median for outside the capital.&lt;/p&gt;
&lt;h2 id=&#34;6-theres-a-split-between-customer--and-product--centric-roles-as-well-as-a-niche-group-of-research-focused-jobs&#34;&gt;6. There’s a split between customer- and product- centric roles, as well as a niche group of research focused jobs:&lt;/h2&gt;
&lt;p&gt;Using topic modelling (&lt;a href=&#34;https://medium.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LDA&lt;/a&gt;) to cluster and group jobs, I could determine 3 emergent topics that weren’t just noise. These were the&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;The Customer centric roles&lt;/strong&gt; — candidates would be delivering insights towards customers and using tools such as ‘dashboards’, ‘excel’, (Power) ‘bi’ and thus providing analytical insights for the stakeholders.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Product and development roles —&lt;/strong&gt; candidates programming languages (‘java’), specific packages used for intensive machine learning (‘&lt;a href=&#34;https://www.tensorflow.org/?hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tensorflow&lt;/a&gt;’, ‘&lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch&lt;/a&gt;’) and more niche areas of data science (‘&lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_language_processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP&lt;/a&gt;’, ‘&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neural’ (networks&lt;/a&gt;)).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Academic and Research —&lt;/strong&gt; candidates would require higher qualifications for these roles such as PhDs. There’s a stronger emphasis on ‘publications’, ‘research’ but also on artificial intelligence and ‘novel’ techniques. These are more challenging roles to get into, but also exciting for the ML-inclined.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This last insight was generated with unsupervised techniques and it is not currently possible to verify the accuracy of those topics. However, they do corroborate other findings in Key Insight 3.&lt;/p&gt;
&lt;h1 id=&#34;iii-methodology&#34;&gt;III. Methodology&lt;/h1&gt;
&lt;p&gt;For this project, I’ve followed the &lt;a href=&#34;http://wiki.gis.com/wiki/index.php/PPDAC_Model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;PPDAC&lt;/strong&gt;&lt;/a&gt; cycle for data science:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*XFvn6RAQ6xsh7qOe3UGl3w.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.1 — PPDAC cycle&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt; — Determine what questions we want to answer using the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2. &lt;strong&gt;Plan&lt;/strong&gt; —What libraries would I use? How many statistical tests should I run and what confidence level should I set in advance? What&lt;/p&gt;
&lt;p&gt;3. &lt;strong&gt;Data&lt;/strong&gt; — Web-scraped job descriptions from Indeed.co.uk, searched for between 24th and 25th November, 2020.&lt;/p&gt;
&lt;p&gt;4. &lt;strong&gt;Analysis&lt;/strong&gt; — Plot salary distributions for London and non-London jobs; different job categories and jobs by which programming language they mention. With an initial alpha of 0.05 I will run several NHST tests and report on results.&lt;/p&gt;
&lt;p&gt;5. &lt;strong&gt;Conclusion&lt;/strong&gt; — Report any insights, recommendations and future steps.&lt;/p&gt;
&lt;h1 id=&#34;1-problem&#34;&gt;1. Problem&lt;/h1&gt;
&lt;p&gt;If we want to get ahead in the data science UK job market, it would be useful to be able to answer the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How many data science jobs advertise salary?&lt;/li&gt;
&lt;li&gt;What is the spread of salaries advertised for data scientist jobs on indeed.co.uk?&lt;/li&gt;
&lt;li&gt;What are the main locations that data scientist roles appear in? (London expected to be the main one)&lt;/li&gt;
&lt;li&gt;What are some of the most frequent words mentioned in the job title?&lt;/li&gt;
&lt;li&gt;Which programming languages are in greatest demand? Do any of the languages correlate with higher salary?&lt;/li&gt;
&lt;li&gt;Is there any relationship between years of experience required and salary?&lt;/li&gt;
&lt;li&gt;What are the main topics emerging from the job descriptions and the title?&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;2-plan&#34;&gt;2. Plan&lt;/h1&gt;
&lt;p&gt;My plan was to scrape data from Indeed.co.uk using &lt;a href=&#34;https://www.crummy.com/software/BeautifulSoup/bs4/doc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BS4&lt;/a&gt; and &lt;a href=&#34;https://www.selenium.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Selenium&lt;/a&gt;. Then I would extract key information such as salary, location and programming languages using &lt;a href=&#34;https://docs.python.org/3/library/re.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;regex&lt;/a&gt; (&lt;strong&gt;reg&lt;/strong&gt;ular &lt;strong&gt;ex&lt;/strong&gt;pressions). I would plot and wrangle data using the &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pandas&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/search?q=seaborn&amp;#43;library&amp;amp;oq=seaborn&amp;#43;library&amp;amp;aqs=chrome..69i57.3040j0j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seaborn&lt;/a&gt; libraries then carry out statistical tests. I would also attempt to build a predictive model for salary using linear regression, and report on the coefficients as measures of the importance of each job feature. Lastly I would use &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt; to look for emerging topics within job descriptions.&lt;/p&gt;
&lt;p&gt;The ideal situation would be to answer all questions stated and to be able to state Insights and &lt;strong&gt;Recommendations&lt;/strong&gt; for every part of my analysis. However, in several areas, I had to report null resorts due to either insufficient data or a lack of a signal, and will have to revisit those questions in future iterations of the project.&lt;/p&gt;
&lt;h1 id=&#34;3-data&#34;&gt;3. Data&lt;/h1&gt;
&lt;p&gt;From previous attempts at web-scraping jobs on Indeed.co.uk, I know that the number of jobs listed at any one time are in the hundreds, so there would likely be a problem of insufficient data for some of the questions I wanted to answer. Another problem was trying to make sure that the search terms I used captured the &lt;em&gt;field&lt;/em&gt; of data science as much as possible, rather than just one specific role within it.&lt;/p&gt;
&lt;p&gt;To tackle those two problems, I retrieved job data from Indeed.co.uk based on 3 separate searches. For each of these, my search results only returned job posts where the &lt;em&gt;title&lt;/em&gt; of the job included the following pairs of words:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data, scientist (DS)&lt;/li&gt;
&lt;li&gt;machine, learning (ML)&lt;/li&gt;
&lt;li&gt;data, analyst (DA)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There were jobs that contained a mixture of 2 of the title words (these duplicates were discarded). Moreover it’s generally known that a &lt;em&gt;data analyst&lt;/em&gt; is doing &lt;em&gt;data science,&lt;/em&gt; an &lt;em&gt;ML engineer&lt;/em&gt; does do &lt;em&gt;some&lt;/em&gt; analysis, a &lt;em&gt;data scientist&lt;/em&gt; does use &lt;em&gt;machine learning,&lt;/em&gt; and so on.&lt;/p&gt;
&lt;h1 id=&#34;4-analysis&#34;&gt;4. Analysis&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Most data science jobs do not directly state their salary&lt;/strong&gt; (Fig 3.2)&lt;strong&gt;:&lt;/strong&gt; about 59% of data science jobs in our sample do not state salary.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1296/1*1r_l1qOGTTQEM2wBlqcZlQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.2: being coy about pay&lt;/p&gt;
&lt;p&gt;From the perspective of a recent data science grad or someone fresh out of a bootcamp, one challenge they’ll face is salary negotiation — particularly daunting when most jobs do not directly state their salary. This applies across the board to all 3 categories, although the gap is wider for DA and ML roles.&lt;/p&gt;
&lt;p&gt;For the bootcamp’s organisers, this makes it even more important that they research salary estimates thoroughly and inform their students of this — to help reach the goal of maximising average salary of bootcamp grads they should also be given help with salary negotiation.&lt;/p&gt;
&lt;h2 id=&#34;2-what-is-the-spread-of-salaries-advertised-for-data-scientist-jobs-on-indeedcouk&#34;&gt;&lt;strong&gt;2. What is the spread of salaries advertised for data scientist jobs on indeed.co.uk?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*VBikCkFtZVypo-y9LDz0JA.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.3: annual salary distribution — regarding the median with a decimal: salaries that stated a range (e.g. £40–45) had their average taken instead of both values. Hence the non-integer median value&lt;/p&gt;
&lt;p&gt;Among the jobs that reported &lt;strong&gt;annual salary&lt;/strong&gt; (Fig 3.3), DA jobs were not as well paid as ML or DS jobs by quite a margin — the median salaries for DS and ML are at least £12k above the median for all data science jobs; &lt;strong&gt;DA roles pay about £20k less!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is fairly solid finding since it’s supported by general background knowledge about the field that analyst roles tend to be less technically specialist and pay less compared to other data science roles. If you’re aiming to maximise salary, then a recommendation might be to prepare your grads to aim for DS and ML jobs. However DA jobs are also the most numerous. They might form a reliable fallback for bootcamp grads not managing to hit targets for the DS and ML roles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. What are the main locations that data scientist roles appear in? (London expected to be the main one)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*2TUTqpFmai30j24maaPtmA.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.4 — dominated by the capital&lt;/p&gt;
&lt;p&gt;As expected, London dominates the country in terms of data science roles. Even with all the mini tech hubs, the emerging Northern cities and the Silicon Fen in Cambridge, London still edges &lt;em&gt;over the entire rest of the country&lt;/em&gt; (Fig 3.4).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*PlTm2V4MSWBJwTheg-gTCQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.5— number of jobs by top 10 location&lt;/p&gt;
&lt;p&gt;The next 9 ‘locations’ with the most data science roles are utterly dwarfed by London (Fig 3.5). The fourth most popular location (as declared in the job) is ‘Home Based’, which is unfortunate for anyone hoping data science jobs might invigorate the North or anywhere that isn’t London.&lt;/p&gt;
&lt;p&gt;The picture becomes even more dire when we consider salary breakdown between the capital and rest of the kingdom. Figure 3.6 shows the annual salaries for the 3 sub-groups in London. The purple and yellow line show the non-London and London median salaries respectively. There is £20k difference between the two!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*aTSpt_0kgjbM32S4BYIDNQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.6— London vs the rest of the country&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. What are some of the most frequent words mentioned in the job title?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A job title can communicate a lot of things. For instance if a role mention “R” in their title (e.g. “Data Scientist with R experience”) you’ll have good reason to ignore that role if you don’t code in R. If we look at the single terms and bigrams (2 word combos) that appear most frequently (Fig 3.7) we can infer the following:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*ChFLF8dmi9cHEs9sAyjECw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.7 — top terms appearing in the title&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;About 14% of jobs are “senior” roles, hence looking for more experienced data scientists. This does not bode well for recent bootcamp graduates. ‘Junior’ roles make up less than 2% of the sample.&lt;/li&gt;
&lt;li&gt;There’s about 10% of roles that are looking for an engineer role, which could either be ‘machine learning engineer’ (most of the occurrences) or ‘software engineer’ (~19 roles in all). These are roles that would we would expect to have much stronger requirements for code development skills.&lt;/li&gt;
&lt;li&gt;There’s very few titles explicitly mentioning a language (e.g. Python is mentioned in less than 2% of titles. To find any trends in programming languages we’ll have to search through the job descriptions.&lt;/li&gt;
&lt;li&gt;Very few jobs advertise by specific area of expertise it seems (e.g. “Natural Language processing”, “public health”). The most frequent case of data science specific expertise being demanded are roles focussed on Computer Vision (13 jobs). Perhaps this would indicate that most roles look for a blend of different skills, but aren’t looking for one single specialty area. This is encouraging for us since our graduates will have some flexibility in what skills to use for leverage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;5. Which programming languages are in greatest demand? Do any of the languages correlate with higher salary?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this question, I went beyond including just programming languages but also techniques, libraries, cloud services and skills. This has greatly increased the usefulness of the findings.&lt;/p&gt;
&lt;p&gt;Figure 3.8 illustrates how the 10 most popular languages and skills compare across the 3 groups. This particular graph can be used to tailor your own portfolio building journey.&lt;/p&gt;
&lt;p&gt;For instance, if you have expertise in R and SQL, you’re more well positioned to aim for a DA roles. If you have experience in Java and are considering pivoting into data science, then focusing on Python and AWS will put you in good stead.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*IR6XEm5D8i8tT0tzWGyiMQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.8— Skills in demand&lt;/p&gt;
&lt;p&gt;If you’re a data bootcamp, you can help inform more clearly which skills grads can leverage towards which roles, so they can optimise their job search. Towards the end of your course, you could create ‘Data Science Profiles’ that learners can gravitate towards depending on the kind of work they would prefer doing. Those that want heavier computational work (so more on the ML side), can attend classes on Docker, Java and AWS. Those drawn more towards analytics can spend more time refining their SQL and other relevant skills.&lt;/p&gt;
&lt;h2 id=&#34;modelling--a-null-result&#34;&gt;Modelling — a null result&lt;/h2&gt;
&lt;p&gt;I next attempted to build a linear regression model for predicting salary, but it would seem &lt;strong&gt;that there wasn’t any way to reliably predict the salary from the features I built&lt;/strong&gt;. After multiple iterations, dropping non-significant features and cross-validating, the summary for the best model is this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model had &lt;strong&gt;root-mean squared error (RMSE) of about £13,000&lt;/strong&gt; on test data, which is a very wide margin for our purposes. So if we took a job post and predicted that the role should be paid around £40 000, on average the answer would be that figure, give or take £13k. Therefore this tool needs more / different data and features before it can be useful.&lt;/li&gt;
&lt;li&gt;The strongest determiner of salary was the presence of the word ‘lead’ in the title — according to the model, the difference between a role that has lead in the title and one that doesn’t is, on average, £23k.&lt;/li&gt;
&lt;li&gt;The weakest feature was whether the title included ‘scientist’— acc to the same model, the difference you’d expect in salary for a role that has the word ‘scientist’ in the title vs one that does not, was about £7k.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also attempted to bin the salary data into bands so that I could attempt to predict (binned into categories such as £20&#39;000–25&#39;000). However, even that didn’t have much predictive power, with the best model having an accuracy of about 10% (given 10 different salary bands). To conclude, the model is nullified and I will attempt to build another in future iterations of the project, when more data is available. However, it might just be the case that the features used have no real relationship with salary.&lt;/p&gt;
&lt;h1 id=&#34;6-is-there-any-relationship-between-years-of-experience-required-and-salary&#34;&gt;6. Is there any relationship between years of experience required and salary?&lt;/h1&gt;
&lt;p&gt;Many new data scientists find it vexing or disappointing when they search for an “entry-level” position and find that it requires 3 years of experience in the field. But do the years of experience required stated in the job ad actually have anything to do with something more concrete, like pay?&lt;/p&gt;
&lt;p&gt;After extracting the required experience in years from job posts using regex and averaging those that gave a range (“2–3 years experience” becomes 2.5), I compared it to salary for those jobs. Jobs with 0 years experience were found by searching for “junior role / data / position” (this is probably the most contentious assumption of the created feature). Unfortunately there were very few jobs I could extract such data for (246 job posts), although the Spearman Ranked Correlation test was significant (&lt;strong&gt;p value &amp;lt; 0.001&lt;/strong&gt;). The &lt;strong&gt;experience and salary have a weak-to-moderate correlation&lt;/strong&gt; (&lt;strong&gt;0.37&lt;/strong&gt;). We must remember that this was based on data mined with regex and is relying on certain assumptions and limitations. Hence this is an important test to re-run in the next project iteration.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1008/1*H5krNpINKuw75UNZKVdm4A.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.9 — how many years??&lt;/p&gt;
&lt;p&gt;Out of those jobs, Figure 3.9 shows the general trend. The largest number of jobs in this group asked for 2 OR 3 years of experience. Hopefully this isn’t representative of the job market, but if it is then as a beginner data scientist, you’d have to find ways to make up for the lack of experience, e.g. by doing freelance work for a while.&lt;/p&gt;
&lt;h1 id=&#34;7-what-are-the-main-topics-emerging-from-the-job-descriptions-and-the-title&#34;&gt;&lt;strong&gt;7. What are the main topics emerging from the job descriptions and the title?&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;Using topic modelling, can we see if there are natural groups within the job descriptions? Can we split apart our data in some semantic way?&lt;/p&gt;
&lt;p&gt;Using Latent Dirichlet Allocation and pyLDAviz (&lt;a href=&#34;https://medium.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here’s my previous post&lt;/a&gt; on it), I determined a few emergent topics of interest. It’s important to note that, since this is an unsupervised approach, there’s a strong chance that the outputs are mostly noise, and not useful insights. However, guided by domain knowledge and other pieces of information in this dataset, we can infer at least 3 useful topics.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Client and Business-centric&lt;/em&gt; — (Fig 3.10) Roles heavily featuring this topic are focused more on delivering insights towards customers and using tools such as dashboards, excel, (power) ‘bi’ and thus providing analytical insights for the stakeholders.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*3JOj8mRch3XLVu9UQfD0Mw.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.10— Client and Business centric&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Development and Deep Learning&lt;/em&gt;— (Fig 3.11) This topic and associated job roles are focussed on development programming languages (‘java’), specific packages used for deep learning (‘tensorflow’, ‘pytorch’), niche areas (‘NLP’, ‘neural’ (networks)) and mentions ‘development’, ‘processing’ and ‘product’. This topic corresponds strongly to a lot of ML jobs.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*--JQ4Os7AO9G6YI-Uyr-Tw.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.11—Development and Deep Learning&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Academic &amp;amp; Scientific&lt;/em&gt; (Fig 3.12)  — there’s a very strong association with this topic and terms such as ‘university’ and ‘research’ — more so than for any other topic! Also the only other topic with a strong association with ‘AI’, ‘novel’, ‘publication’ and ‘academic’ is Topic 1 — Deep Learning and Development.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*6jiWe7njEo74vkwfvWEQbg.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.12 — Academic and Scientific roles&lt;/p&gt;
&lt;h1 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;My plan is to repeat this project in late Q1 2021 with a fresh batch of job post data and to improve the functionality of the web scraper to be able to detect tags for things such as “Remote working”. I hope that this exploratory analysis proves useful to some people, although I repeat that all findings should be considered in light of the unstructured and semi-rigorous nature of the work. None of these findings can/should be interpreted as conclusive, only preliminary. As said before, I appreciate any feedback and any claps! If I had to summarise the most important advice for aspiring data scientists in one bullet point it would be this:&lt;/p&gt;
&lt;p&gt;Invest &lt;strong&gt;most time into mastering Python and SQL&lt;/strong&gt;, &lt;strong&gt;play the long game&lt;/strong&gt; and &lt;strong&gt;lower your initial expectations&lt;/strong&gt; of what pay or job you’ll get, with the realistic hope that 2 years of building experience in your less-than-ideal job. Don’t spend too much time initially on niche, fancy areas like deep learning or NLP, you can revisit those at a latter stage. Knowledge of the basics, of how to solve problems and work experience are what pay off most.&lt;/p&gt;
&lt;p&gt;Thank you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/post/2020-12-01-r-rmarkdown/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>/post/2020-12-01-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/2020-12-01-r-rmarkdown/index.en_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Latent Dirichlet Allocation: Intuition, math, implementation and visualisation with pyLDAvis</title>
      <link>/publication/lda_medium/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/publication/lda_medium/</guid>
      <description>&lt;p&gt;TL;DR — &lt;a href=&#34;https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt; (LDA, sometimes LDirA/LDiA) is one of the most popular and interpretable generative models for finding &lt;strong&gt;topics in text data&lt;/strong&gt;. I’ve provided an &lt;a href=&#34;https://nbviewer.jupyter.org/github/Ioana-P/MLEng_vs_DScientist_analysis/blob/master/2_Topic_modelling.ipynb#topic=0&amp;amp;lambda=1&amp;amp;term=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example notebook&lt;/a&gt; based on web-scraped job description data. Although running LDA on a canonical dataset like &lt;a href=&#34;https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;20Newsgroups&lt;/a&gt; would’ve provided &lt;a href=&#34;https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;clearer topics&lt;/a&gt; , it’s important to witness how difficult topic identification can be “in the wild”, and how you might not actually find clear topics — with unsupervised learning, you are &lt;em&gt;never guaranteed to find an answer!&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;: the greatest aid to &lt;em&gt;my&lt;/em&gt; understanding was Louis Serrano’s two videos on LDA (2020). A lot of the intuition section is based on his explanation, and I would urge you to visit his &lt;a href=&#34;https://www.youtube.com/watch?v=T05t-SqKArY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt; for a more thorough dissection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*Xs1Xe1Hh4P6IGyWN8fImXw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.0 — the LDA “machine” producing documents&lt;/p&gt;
&lt;h1 id=&#34;contents&#34;&gt;Contents:&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;#f1d8&#34;&gt;Intuition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#7b8f&#34;&gt;Maths&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#b9d6&#34;&gt;Implementation and visualisation&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;intuition&#34;&gt;Intuition&lt;/h1&gt;
&lt;p&gt;Let’s say that you have a collection of different news articles (your &lt;em&gt;corpus&lt;/em&gt; of &lt;em&gt;documents&lt;/em&gt;), and you suspect that there are several topics that come up frequently within said corpus — your goal is to find out what they are! To get there you make a few &lt;strong&gt;key assumptions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;The d&lt;/em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Distributional_semantics#:~:text=The%20distributional%20hypothesis%20suggests%20that,occur%20in%20similar%20linguistic%20contexts.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;istributional hypothesis&lt;/em&gt;&lt;/a&gt;&lt;em&gt;:&lt;/em&gt; Words that appear together frequently are likely to be close in meaning;&lt;/li&gt;
&lt;li&gt;each topic is a mixture of different words (Fig 1.1);&lt;/li&gt;
&lt;li&gt;each document is a mixture of different topics (Fig 1.2).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*bgPL1Ex8dfxBSM7bSE3HlA.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.1 — Topics as a mixture of words&lt;/p&gt;
&lt;p&gt;In Fig 1.1 you’ll notice that the topic “Health &amp;amp; Medicine” has various words associated with it to &lt;em&gt;varying degrees&lt;/em&gt; (“cancer” is more strongly associated than “vascular” or “exercise”). Note that different words can be associated with different topics, as with the word “cardio”.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*-dW-PbkYomLrP6XtNTYwHA.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.2 — Document as a mixture of topics&lt;/p&gt;
&lt;p&gt;In Fig 1.2 you’ll see that a single document can pertain to multiple topics (as colour-coded on the left). Words like “injury” and “recovery” might also belong to multiple topics (hence why I’ve coloured them in more than one colour).&lt;/p&gt;
&lt;p&gt;Now LDA is a &lt;em&gt;generative model&lt;/em&gt; — it tries to determine the underlying mechanism that &lt;em&gt;generates&lt;/em&gt; the articles and the topics. Think of it as if there’s a machine with particular settings that spits out articles, but we can’t see the machine’s settings, only what it produces. LDA creates a set of machines with different settings and selects the one that gives the best-fitting results (Serrano, 2020). Once the best one is found, we take a look at its “settings” and we deduce the topics from that.&lt;/p&gt;
&lt;p&gt;So what are these &lt;em&gt;settings&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;First, we have something called the &lt;em&gt;Dirichlet&lt;/em&gt; (pronounced like dee-reesh-lay) &lt;em&gt;prior&lt;/em&gt; of the topics. This is a number that says how &lt;em&gt;sparse&lt;/em&gt; or how &lt;em&gt;mixed&lt;/em&gt; up our topics are. In L Serrano’s video (which I highly recommend!) he illustrates how visually you can think of this as a triangle (Fig 1.3) where the dots represent the documents and their position with respect to the corners (i.e. the topics) represents the how they’re related to each of the topics (2020). So a dot that is very close to the “Sports” vertex will be almost entirely about sport.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*46pYVxXIOAL7qd40Bs_xHQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.3 — Dirichlet distribution of topics&lt;/p&gt;
&lt;p&gt;In the lefthand triangle the documents are fairly separated, most of them neatly tucked into their corners (this corresponds to a low Dirichlet prior, alpha&amp;lt;1); on the right they are in the middle and represent a more even mix of topics (a higher Dirichlet prior, alpha&amp;gt;1). Look at the document in Fig 1.2 and, given the mix of topics, have a think about where you think it would be placed in the triangle on the right (my answer is that it’d be the dot &lt;em&gt;just above&lt;/em&gt; the one closest to the Sports corner).&lt;/p&gt;
&lt;p&gt;Second, we have the Dirichlet prior of the &lt;em&gt;terms&lt;/em&gt; (all the words in our vocabulary). This number (whose name is &lt;em&gt;beta)&lt;/em&gt; has almost exactly the same function as alpha — except that it determines how the &lt;strong&gt;topics&lt;/strong&gt; are distributed amongst the &lt;strong&gt;terms&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*ctgYvHaDDkcDKAzYcVigHg.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.4 Dirichlet distribution of terms; the numbers are proportional to how much each word is associated with each respective topic&lt;/p&gt;
&lt;p&gt;As we said before, the topics are assumed to be mixtures (more precisely, &lt;em&gt;distributions&lt;/em&gt;) of different terms. In Fig 1.4 “Sports” is mostly drawn towards “injury”. “Health&amp;amp;Medicine” is torn between “cardio” and “injury” and has no association with the term “pray”.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But wait, our vocabulary doesn’t consist of just 3 words!&lt;/em&gt; You’re right! We could have a vocabulary of &lt;em&gt;4 words&lt;/em&gt; (as shown in Fig 1.5)! Trouble is that visualising a typical vocabulary of &lt;em&gt;N&lt;/em&gt; words (where &lt;em&gt;N&lt;/em&gt; could be 10&#39;000) would require a &lt;a href=&#34;https://en.wikipedia.org/wiki/Simplex#The_standard_simplex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;generalised version of the triangle shape,&lt;/a&gt; but in &lt;em&gt;N — 1&lt;/em&gt; dimensions (the term for this is an n-1 &lt;em&gt;simplex&lt;/em&gt;). This is where the visuals stop and we trust that the maths of higher dimensions will function as expected. This also applies to the topics — very often we’ll find ourselves with more than 3 topics.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*iq3bjiBg_Pchh0upPmnmMQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.5 — which topic is the red one, based on the distribution of terms?&lt;/p&gt;
&lt;p&gt;An important clarification: in LDA we start with values of alpha and beta as hyperparameters, but these numbers &lt;em&gt;only&lt;/em&gt; tell us whether our dots (documents / topics) are &lt;strong&gt;generally&lt;/strong&gt; concentrated in the middle of their triangles or closer to the corners. The &lt;em&gt;actual positions&lt;/em&gt; within the triangle (simplex) are guessed by the machine — the guesswork is not random, it’s heavily weighted by the Dirichlet priors.&lt;/p&gt;
&lt;p&gt;So the machine creates the two Dirichlet distributions, &lt;em&gt;distributes&lt;/em&gt; the documents and topics on them and then &lt;em&gt;generates&lt;/em&gt; documents based on those distributions (Fig 1.6). So, how does the last step happen, the &lt;em&gt;generation&lt;/em&gt; part?&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*Xs1Xe1Hh4P6IGyWN8fImXw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.6 — the LDA “machine” producing documents&lt;/p&gt;
&lt;p&gt;Remember at the start we said that topics are seen as mixtures / distributions of words and documents as mixtures / distributions of topics? Going from left to right in Figure 1.7 we start with a document, somewhere in the triangle, torn between our 3 topics. If it’s near the “Sports” corner, this means that the document will be &lt;em&gt;mostly about Sports&lt;/em&gt;, with some mentions of “Religion” and “Health&amp;amp;Medicine”. So we know the topic composition of the document → therefore we can estimate what &lt;em&gt;words&lt;/em&gt; will come up. We will be sampling (i.e. randomly pulling out) words mostly from Sports, some from Health&amp;amp;Medicine and a very small amount from Religion (Fig 1.7). Here’s a question for you: looking at the triangle at the bottom of Fig 1.7, do you think &lt;em&gt;word 2&lt;/em&gt; will come up or not?&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*hDZIC8V8IyX-otJ1eblCuw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.7 — how the two Dirichlet distributions feed into our document generation&lt;/p&gt;
&lt;p&gt;The answer is that &lt;strong&gt;it might&lt;/strong&gt;: remember that topics are mixtures of words. You might be thinking that &lt;em&gt;word 2&lt;/em&gt; is very strongly related to the yellow (Religion) topic, and since this topic is very sparse in this document &lt;em&gt;word 2&lt;/em&gt; won’t come up as much. But remember that a. &lt;em&gt;word 2&lt;/em&gt; is also associated with the blue, Sports topic and b. the words are sample probabilistically, so every word has some non-zero chance of appearing.&lt;/p&gt;
&lt;p&gt;The words in our final, generated document (on the right end of Fig 1.7) will be compared to the words in the original documents. We won’t get the same document, BUT when we compare a range of different LDA “machines” with a range of different distributions, we find that one of them was closer to generating the document than the others were and that’s the LDA model that we choose.&lt;/p&gt;
&lt;h1 id=&#34;maths&#34;&gt;Maths&lt;/h1&gt;
&lt;p&gt;A normal statistical language model assumes that you can generate a document by sampling from a probability distribution over words, i.e. for each word in our vocabulary there is an associated probability of that word appearing.&lt;/p&gt;
&lt;p&gt;LDA adds a layer of complexity over this arrangement. It assumes a list of topics, &lt;em&gt;k&lt;/em&gt;. Each document &lt;em&gt;m&lt;/em&gt; is a probability distribution over these &lt;em&gt;k&lt;/em&gt; topics, and each topic is a probability distribution over all the different terms in our vocabulary &lt;em&gt;V&lt;/em&gt;. That is to say that each word has various probabilities of appearing in each topic.&lt;/p&gt;
&lt;p&gt;The full probability formula that generates a document is in Figure 2.0 below. If we break this down, on the right hand side we have three product sums:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dirichlet distribution of topics over terms:&lt;/strong&gt; (corresponds to Fig 1.4 and 1.5) for each topic &lt;em&gt;i&lt;/em&gt; amongst &lt;em&gt;K topics&lt;/em&gt;, what is the probability distribution of words for &lt;em&gt;i.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dirichlet distribution of documents over topics:&lt;/strong&gt; (corresponds to Fig 1.3) for each document &lt;em&gt;j&lt;/em&gt; in our corpus of size &lt;em&gt;M,&lt;/em&gt; what is the probability distribution of topics for &lt;em&gt;j.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Probability of a topic appearing given a document X the probability of a word appearing given a topic:&lt;/strong&gt; (corresponding to the two rectangles in Fig 1.7) how likely is it that certain topics, &lt;em&gt;Z,&lt;/em&gt; appear in this document and then how likely is that certain words, &lt;em&gt;W,&lt;/em&gt; appear given those topics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*pUTv6gS_8GDQodj4TlGTgw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.0 — LDA formula&lt;/p&gt;
&lt;p&gt;The first two sums contain &lt;strong&gt;symmetric&lt;/strong&gt; Dirichlet distributions which are prior probability distributions for our documents and our topics (Fig 2.1 shows a set of general Dirichlet distributions, including symmetric ones).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1280/1*YJbCG2oZI6prRgIBmHiQtg.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.1 — By Empetrisor — Own work, CC BY-SA 4.0, &lt;a href=&#34;https://commons.wikimedia.org/w/index.php?curid=49908662&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://commons.wikimedia.org/w/index.php?curid=49908662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The 3rd sum contains two multinomial distributions, one over topics and one over words — i.e. we sample topics from a probability distribution of them and then for each topic instance we sample words from a probability distribution of words for that particular topic.&lt;/p&gt;
&lt;p&gt;As was mentioned at the end of the Intuition section, using the final probability we try to generate the same distribution of words as the one that we get in our original documents. The probability of achieving this is &lt;em&gt;very, very low&lt;/em&gt;, but for some values of alpha and beta the probability will be less low.&lt;/p&gt;
&lt;h2 id=&#34;interpreting-an-lda-model-and-its-topics&#34;&gt;Interpreting an LDA model and its topics&lt;/h2&gt;
&lt;p&gt;What metrics do we use for finding our latent topics? As Shirley and Sievert note:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“To interpret a topic, one typically examines a ranked list of the most probable terms in that topic, […]. The problem with interpreting topics this way is that common terms in the corpus often appear near the top of such lists for multiple topics, making it hard to differentiate the meanings of these topics.” (2014)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is exactly the problem we’ve stumbled into in the next section, &lt;em&gt;Implementation&lt;/em&gt;. Therefore we use an alternative metric for interpreting our topics — &lt;em&gt;relevance&lt;/em&gt; (Shirley and Sievert, 2014).&lt;/p&gt;
&lt;h2 id=&#34;relevance&#34;&gt;Relevance&lt;/h2&gt;
&lt;p&gt;This is an adjustable metric that balances a term’s frequency in a particular topic against the term’s frequency across the whole corpus of documents.&lt;/p&gt;
&lt;p&gt;In other words, if we have a term that’s quite popular in a topic, relevance allows us to gauge how much of its popularity is due to it being very specific to that topic and how much of it is due to it just being a work that appears &lt;em&gt;everywhere.&lt;/em&gt; An example of the latter would be “learning” in the job description data. When we adjust relevance with a lower lambda (i.e. penalising terms that just happen to be frequent across &lt;strong&gt;all&lt;/strong&gt; topics), we see that “learning” is not that special a term, and it only comes up frequently because of its prevalence across the corpus.&lt;/p&gt;
&lt;p&gt;The mathematical definition of relevance is:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1172/0*tL0f-BtwU3oSv-8-&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;r —&lt;/em&gt; relevance&lt;/li&gt;
&lt;li&gt;&lt;em&gt;⍵ —&lt;/em&gt; a term in our vocabulary&lt;/li&gt;
&lt;li&gt;&lt;em&gt;k —&lt;/em&gt; a topic amongst the ones our LDA has produced&lt;/li&gt;
&lt;li&gt;&lt;em&gt;λ —&lt;/em&gt; the adjustable weight parameter&lt;/li&gt;
&lt;li&gt;𝝓kw — probability of a term appearing in a particular topic&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;w —&lt;/em&gt; the probability of a term appearing inside the corpus as a whole&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apart from lambda, &lt;em&gt;λ,&lt;/em&gt; all the terms are derived from the LDA data and model. We adjust lambda in the next section to help us derive more useful insights. The original paper authors kept lambda in the range of 0.3 to 0.6 (Shirley and Sievert, 2014).&lt;/p&gt;
&lt;h1 id=&#34;implementation-and-visualisation&#34;&gt;Implementation and visualisation&lt;/h1&gt;
&lt;p&gt;The implementation of sklearn’s LatentDirichletAllocation model follows the pattern of most sklearn models. In my &lt;a href=&#34;https://nbviewer.jupyter.org/github/Ioana-P/MLEng_vs_DScientist_analysis/blob/master/2_Topic_modelling.ipynb#topic=0&amp;amp;lambda=1&amp;amp;term=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notebook&lt;/a&gt;, I:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pre-processed my text data,&lt;/li&gt;
&lt;li&gt;Vectorised it (resulting in a document-term matrix),&lt;/li&gt;
&lt;li&gt;Fit_transformed it using LDA and then&lt;/li&gt;
&lt;li&gt;Inspected the results to see if there are any emergent, identifiable topics.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The last part is highly subjective (remember this is &lt;em&gt;unsupervised learning&lt;/em&gt;) and is not guaranteed to reveal anything really interesting. Furthermore the ability to identify topics (like clusters) depends on your domain knowledge of the data. I recommend also altering the alpha and beta parameters to match your expectations of the text data.&lt;/p&gt;
&lt;p&gt;The data I’m using is job post description data from indeed.co.uk. The dataframe has many other attributes than text, including whether I used the search terms “data scientist”, “data analyst” or “machine learning engineer”. Can we find some of the original search categories in our LDA topics?&lt;/p&gt;
&lt;p&gt;In the gist below you’ll see that I’ve vectorised my data and passed it to an LDA model (this happens under the hood of the data_to_lda function).&lt;/p&gt;
&lt;p&gt;Running this code and the print_topics function will produce something like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Topics found via LDA on Count Vectorised data for ALL categories:  
  
Topic #1:  
software; experience; amazon; learning; opportunity; team; application; business; work; product; engineer; problem; development; technical; make; personal; process; skill; working; science  
  
Topic #2:  
learning; research; experience; science; team; role; work; working; model; skill; deep; please; language; python; nlp; quantitative; technique; candidate; algorithm; researcherTopic #3:  
learning; work; team; time; company; causalens; business; high; platform; exciting; award; day; development; approach; best; holiday; fund; mission; opportunity; problem  
  
Topic #4:  
client; business; team; work; people; opportunity; service; financial; role; value; investment; experience; firm; market; skill; management; make; global; working; support...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The “print_topics” function gives the terms for each topic in decreasing order of probability, which &lt;strong&gt;can&lt;/strong&gt; be informative. It’s at this stage that we can &lt;strong&gt;start&lt;/strong&gt; trying to label the emergent, latent topics from our model. For instance, Topic 1 seems to be related mildly related to ML engineer skills and requirements (the mention of “amazon” relates to using AWS — this is something I found from the EDA stage of the project in another notebook); meanwhile, Topic 4 clearly has a more client-facing or business-oriented theme, given terms like “market”, “financial”, “global”.&lt;/p&gt;
&lt;p&gt;Now those two categories might seem a bit far-fetched to you and that’s a fair criticism. You may also have noticed that using this method for topic determination is hard. So, let’s turn to pyLDAvis!&lt;/p&gt;
&lt;h2 id=&#34;pyldavis&#34;&gt;pyLDAvis&lt;/h2&gt;
&lt;p&gt;Using pyLDAvis, the LDA data (which in our case, was 10-dimensional) has been decomposed via PCA (principal component analysis) to be only 2-dimensional. Thus it has been flattened for the purposes of visualisation. We have ten circles and the center of each circle represents the position of our topic in the latent feature space; the distances between topics illustrates how (dis)similar the topics are and the area of the circles is proportional to how many documents feature each topic.&lt;/p&gt;
&lt;p&gt;Below I’ve shown how you insert an already trained sklearn LDA model in pyLDAvis. Thankfully the &lt;a href=&#34;https://github.com/bmabey/pyLDAvis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;people responsible for adapting the original LDAvis&lt;/a&gt; (which was R model) to python made it communicate efficiently with sklearn.&lt;/p&gt;
&lt;p&gt;And in Fig 3.0 is the plot we generate:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*e9Fj031z3H1s_eNx_KnfWg.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.0 — pyLDAvis interactive plot&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting pyLDAvis plots&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The LDAvis plot comes in two parts — a 2-dimensional ‘flattened’ replotting of our n-dimensional LDA data and an interactive, varying horizontal bar-chart of term distributions. Both of these are shown in Fig A1.0. One important feature to note is that the right-hand bar chart shows the terms in a topic in &lt;em&gt;decreasing order of relevance&lt;/em&gt;, but the bars indicate the frequency of the terms. The red section represents the term frequency purely within the particular topic; the red and blue represent the overall term frequency within the corpus of documents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adjusting&lt;/strong&gt; &lt;em&gt;λ (lambda)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we set λ equal to 1, then our relevance is given purely by the probability of the word to that topic. Setting it to 0 will result in our relevance being dictated by specificity of that word to the topic — this is because the right hand term divides the probability of a term appearing in a particular topic divided by the probability of the word appearing generally — thus, highly frequent words (such as ‘team’, ‘skill’, ‘business’) will be downgraded heavily in relevance when we have a lower &lt;em&gt;λ&lt;/em&gt; value.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*gZJYETiTTlLPyi2VsXam9Q.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.1 — setting lambda to 1&lt;/p&gt;
&lt;p&gt;In Fig 3.1 &lt;em&gt;λ&lt;/em&gt; was set to 1 and you can see that the terms tend to match the ones that dominate across the board generally (i.e. like in our print-outs of the most popular terms for each topic). This was only done for topic 1, but when I changed topic the distribution of top-30 most relevant terms barely changed at all!&lt;/p&gt;
&lt;p&gt;Now, in Fig 3.2 &lt;em&gt;λ&lt;/em&gt; was set to 0 and the terms changed completely!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*vHiv2kJNqRAZdsC23_O3sg.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.2 — lambda set to 0&lt;/p&gt;
&lt;p&gt;Now we have highly specific terms, but pay attention to the scale at the top — the most relevant word appears about 60 times. That’s quite a come down after over 6000! Also, these words won’t necessarily tell us anything interesting. If you select a different topic with this lambda value you will keep getting junk terms that aren’t necessarily that important.&lt;/p&gt;
&lt;p&gt;In Fig 3.3 I’ve set lambda to 0.6 and I am exploring topic 2. Right off the bat there is a significant theme here surrounding engineer work, with terms like “aws”, “cloud” and “platform”.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*6Z1RjZ39WG4OCe6uyFE4uA.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.3 — lambda = 0.6&lt;/p&gt;
&lt;p&gt;Another great thing that you can do with pyLDAvis is visually inspect the conditional topic distribution given a word, simply by hovering over the word (Fig 3.4). Below we can see just how much “NLP” is split amongst several topics — not a lot! This gives me further reason to believe that topic 6 is focused on NLP and text-based work (terms like “speech”, “language”, “text” also help in that regard). An interesting insight for me is the fact that “research” and “PhD” co-occur so strongly in this topic.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*lsD8XKNR7YSUlqkcVp-ZjA.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 3.4 — conditional topic distribution for “NLP”&lt;/p&gt;
&lt;p&gt;Does this mean that NLP-focussed roles in the industry demand higher education than other roles? Do they demand previous research experience more often than other roles? Are NLP roles perhaps more fixated on experimental techniques and thus require someone with knowledge of the cutting edge?&lt;/p&gt;
&lt;p&gt;While the interactive plot generated cannot deliver concrete answers, what it can do is provide us with a starting position for further investigation. If you’re in an organisation where you can run topic modelling, you can use LDA’s latent themes to inform survey-design, A/B testing or even correlate it with other available data to find interesting correlations!&lt;/p&gt;
&lt;p&gt;I wish you the best of luck in topic modelling. If you’ve enjoyed this lengthy read, please give me as many claps as you think are appropriate. If you have knowledge of LDA and think I’ve gotten something &lt;strong&gt;even partially wrong&lt;/strong&gt; please leave me a comment (feedback is a gift and all that)!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Serrano L. (2020). Accessed online: &lt;a href=&#34;https://www.youtube.com/watch?v=T05t-SqKArY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Latent Dirichlet Allocation (Part 1 of 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sievert C. and Shirley K (2014). &lt;em&gt;LDAvis: A method for visualizing and interpreting topics.&lt;/em&gt; Accessed online: &lt;a href=&#34;https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>1 | Basics of Self-Attention</title>
      <link>/publication/self_attention_1/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/publication/self_attention_1/</guid>
      <description>&lt;p&gt;TL;DR — Transformers are an exciting and (&lt;strong&gt;relatively&lt;/strong&gt;) new part of Machine Learning (ML) but there are a &lt;strong&gt;lot&lt;/strong&gt; of concepts that need to be broken down before you can understand them. This is the first post in a column I’m writing about them. Here we focus on how the basic self-attention mechanism works, which is the first layer of a Transformer model. Essentially for each input vector Self-Attention produces a vector that is the weighted sum over the vectors in its neighbourhood. The weights are determined by the relationship or &lt;em&gt;connectedness&lt;/em&gt; between the words. This column is aimed at ML novices and enthusiasts who are curious about what goes on under the hood of Transformers.&lt;/p&gt;
&lt;h1 id=&#34;contents&#34;&gt;Contents:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#cce2&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2beb&#34;&gt;Self-Attention — the math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c2e8&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;Transformers are an ML architecture that have been used successfully in a wide variety of NLP tasks, especially sequence to sequence (seq2seq) ones such as machine translation and text generation. In seq2seq tasks, the goal is to take a set of inputs (e.g. words in English) and produce a desirable set of outputs (- the same words in German). Since their inception in 2017, they’ve usurped the dominant architecture of their day (&lt;a href=&#34;https://en.wikipedia.org/wiki/Long_short-term_memory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LSTMs&lt;/a&gt;) for seq2seq and have become almost ubiquitous in any news about NLP breakthroughs (for instance OpenAI’s &lt;a href=&#34;https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-2 even appeared in mainstream&lt;/a&gt; media!).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/984/1*pblofc3psQrBkvXI4Jfxog.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 1.1 — machine translation (EN → DE)⁴&lt;/p&gt;
&lt;p&gt;This column is intended as a very gentle, gradual introduction to the math, code and concept behind Transformer architecture. There’s no better place to start with than the attention mechanism because:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The most basic transformers rely purely on attention &lt;strong&gt;mechanisms³.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;2-self-attention--the-math&#34;&gt;2. Self-Attention — the math&lt;/h1&gt;
&lt;p&gt;We want an ML system to learn the important relationships between words, similar to the way a human being understands words in a sentence. In Fig 2.1 you and I both know that “The” is referring to “animal” and thus should have a strong connection with that word. As the diagram’s colour coding shows, this system knows that there is some connection between “animal”, “cross”,“street” and “the” because they’re all &lt;em&gt;related&lt;/em&gt; to “animal”, the subject of the sentence. This is achieved through &lt;em&gt;Self-Attention.⁴&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*9XxSNAGInd3rbwTE_AwrQA.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.1 — which words does “The” pay &lt;strong&gt;&lt;em&gt;attention&lt;/em&gt;&lt;/strong&gt; to?⁴&lt;/p&gt;
&lt;p&gt;At its most basic level, Self-Attention is a process by which one sequence of vectors &lt;em&gt;x&lt;/em&gt; is &lt;strong&gt;encoded&lt;/strong&gt; into another sequence of vectors &lt;em&gt;z&lt;/em&gt; (Fig 2.2). Each of the original vectors is just a &lt;strong&gt;block of numbers&lt;/strong&gt; that &lt;strong&gt;represents a word.&lt;/strong&gt; Its corresponding &lt;em&gt;z&lt;/em&gt; vector represents both the original word &lt;em&gt;and&lt;/em&gt; its &lt;strong&gt;relationship&lt;/strong&gt; with the other words around it.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*qeY6mWlzwkCIl2LhPN0zZQ.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.2: sequence of input vectors &lt;em&gt;x&lt;/em&gt; getting turned into another equally long sequence of vectors &lt;em&gt;z&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Vectors represent some sort of thing in a &lt;em&gt;space,&lt;/em&gt; like the flow of water particles in an ocean or the effect of gravity at any point around the Earth. You &lt;em&gt;can&lt;/em&gt; think of words as vectors in the total space of words. The direction of each word-vector &lt;em&gt;means&lt;/em&gt; something. Similarities and differences between the vectors correspond to similarities and differences between the words themselves (I’ve written about the subject before &lt;a href=&#34;https://medium.com/analytics-vidhya/ideas-for-using-word2vec-in-human-learning-tasks-1c5dabbeb72e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Let’s just start by looking at the first three vectors and only looking in particular at how the vector &lt;em&gt;x2&lt;/em&gt;, our vector for “cat”, gets turned into &lt;em&gt;z2&lt;/em&gt;. All of these steps will be repeated for &lt;em&gt;each&lt;/em&gt; of the input vectors.&lt;/p&gt;
&lt;p&gt;First, we multiply the vector in our spotlight, &lt;em&gt;x2&lt;/em&gt;, with all the vectors in a sequence, &lt;em&gt;including itself&lt;/em&gt;. We’re going to do a product of each vector and the &lt;em&gt;transpose&lt;/em&gt; (the diagonally flipped version) of &lt;em&gt;x2&lt;/em&gt; (Fig 2.3). This is the same as doing a dot product and you can think of a dot product of two vectors as a measure of &lt;strong&gt;how similar they are.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*dVJGPnBgZAFy8MorveslUQ.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.3: transposed multiplication (superscript “T” = “transposed”)&lt;/p&gt;
&lt;p&gt;The dot product of two vectors is proportional to the cosine of the angle between them (Fig 2.4) — so the more closely they align in direction, the larger the dot product. If they were pointing in the exact same direction then the angle A would be 0⁰ and a cosine of 0⁰ is equal to 1. If they were pointing in opposite directions (so that A = 180⁰) then the cosine would be -1.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*2c4vsG2yNRBQL8xsIYKuew.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.4 — dot product of two vectors&lt;/p&gt;
&lt;p&gt;As an aside, note that the &lt;em&gt;operation&lt;/em&gt; we use to get this product between vectors is a hyperparameter we can choose. The dot product is just the simplest option we have and the one that’s used in &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Attention Is All You Need&lt;/em&gt;&lt;/a&gt;&lt;em&gt;³&lt;/em&gt; (AIAYN)&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you want an additional intuitive perspective on this, &lt;a href=&#34;http://www.peterbloem.nl/blog/transformers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bloem’s&lt;/a&gt;¹ post discusses how self-attention is analogous to the way recommender systems determine the similarity of movies or users.&lt;/p&gt;
&lt;p&gt;So we put one word under the spotlight at a time and determine its output from its neighbourhood of words. Here we’re only looking at the words before and after but we could choose to widen that window in the future.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/900/1*RN9sHNRPhQu2atGXzTW5zg.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.5 — raw weights for each j-th vector&lt;/p&gt;
&lt;p&gt;If the spotlit word is “cat”, the sequence of words we’re going over is “the”, “cat”, “sat”. We’re asking &lt;strong&gt;how much attention the word “&lt;em&gt;cat”&lt;/em&gt; should pay to “&lt;em&gt;the”, “cat”&lt;/em&gt; and “&lt;em&gt;sat” respectively&lt;/em&gt;&lt;/strong&gt; (similar to what we see in Fig 2.1).&lt;/p&gt;
&lt;p&gt;Multiplying the transpose of our spotlit word vector and the sequence of words around it will give us a set of 3 &lt;em&gt;raw weights&lt;/em&gt; (Fig 2.5)&lt;em&gt;.&lt;/em&gt; Each weight is proportional to how connected the two words are in meaning. We need to then normalise them so they are easier to use going ahead. We’ll do this using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Softmax_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;softmax formula (Fig 2.6).&lt;/a&gt; This converts a sequence of numbers to be within the range of 0, 1 where each output is proportional to the &lt;em&gt;exponential of the input number&lt;/em&gt;. This makes our weights much easier to use and interpret.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1068/1*FM5PaDrHI31yoE8AwvMAWw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.6: normalising raw weights via softmax function&lt;/p&gt;
&lt;p&gt;Now we take our normalised weights (one per every vector in the &lt;em&gt;j&lt;/em&gt; sequence), multiply them respectively with the &lt;em&gt;x&lt;/em&gt; input vectors, sum the products and bingo! We have an output &lt;em&gt;z&lt;/em&gt; vector, (Fig 2.5)! This is, of course, the output vector &lt;strong&gt;just&lt;/strong&gt; for x2 (“cat”) — this operation will be repeated for every input vector in &lt;em&gt;x&lt;/em&gt; until we get the output sequence we saw in Fig 2.2.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*Q1d4gzdBleLgcMUrI58D8g.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.7: Final operation to get our new sequence of vectors, &lt;em&gt;z&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This explanation so far may have raised some questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aren’t the weights we calculated highly dependent on how we determined the original input vectors?&lt;/li&gt;
&lt;li&gt;Why are we relying on the &lt;em&gt;similarity&lt;/em&gt; of the vectors? What if we want to find a connection between two ‘dissimilar’ words, such as the object and subject of “The cat sat on the matt”?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next post, we’ll address these questions. We’ll transform each vector for each of its different uses and thus define relationships between words &lt;em&gt;more precisely&lt;/em&gt; so that we can get an output more like Fig 2.8.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/1400/1*al_9j5AzCoqPaTUMjFRkjQ.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Fig 2.8 — which words is “cross” paying attention to in the &lt;strong&gt;orange column&lt;/strong&gt; vs the &lt;strong&gt;pink&lt;/strong&gt; one?&lt;/p&gt;
&lt;p&gt;I hope you’ve enjoyed this post and I appreciate any amount of claps. Feel free to leave any feedback (positive or constructive) in the comments and I’ll aim to take it onboard as quickly as I can.&lt;/p&gt;
&lt;p&gt;The people who helped my understanding the most and to whom I am very grateful are Peter Bloem (his &lt;a href=&#34;http://www.peterbloem.nl/blog/transformers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; is a great start if, like me, you prefer a math-first approach to Machine Learning¹ ) and Jay Alammar (if you want a top-down view to start with, I recommend &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his article&lt;/a&gt;²).&lt;/p&gt;
&lt;h1 id=&#34;3-references&#34;&gt;3. References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Alammar J. &lt;em&gt;The Illustrated Transformer.&lt;/em&gt; (2018)  &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt; [accessed 27th June 2020]&lt;/li&gt;
&lt;li&gt;Bloem P. &lt;em&gt;Transformers from Scratch.&lt;/em&gt; (2019) &lt;a href=&#34;http://www.peterbloem.nl/blog/transformers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.peterbloem.nl/blog/transformers&lt;/a&gt; .[accessed 27th June 2020]&lt;/li&gt;
&lt;li&gt;Vaswani A. et al. Dec 2017. &lt;em&gt;Attention is all you need&lt;/em&gt;. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. &lt;a href=&#34;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/a&gt; [accessed 27th June 2020]. &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1706.03762&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Vaswani A. et al. Mar 2018 &lt;a href=&#34;https://arxiv.org/abs/1803.07416&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:1803.07416&lt;/a&gt; . &lt;a href=&#34;https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb?authuser=2#scrollTo=OJKU36QAfqOC&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interactive notebook&lt;/a&gt;: [accessed 29th June 2020]&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis with Pytorch</title>
      <link>/project/sentiment-analysis-modelling-with-pytorch/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/sentiment-analysis-modelling-with-pytorch/</guid>
      <description>&lt;h1 id=&#34;sentiment-analysis-in-pytorch&#34;&gt;Sentiment Analysis in Pytorch&lt;/h1&gt;
&lt;p&gt;Training and deploying a Sentiment Analysis model in Pytorch using AWS Sagemaker. The data used was IMDB&amp;rsquo;s Movie review dataset (&lt;a href=&#34;https://ai.stanford.edu/~amaas/data/sentiment/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ai.stanford.edu/~amaas/data/sentiment/)&lt;/a&gt;, which consists of 50,000 instances of binary labelled movie reviews.&lt;/p&gt;
&lt;h5 id=&#34;how-do-we-take-a-machine-learning-model-and-put-it-out-there-to-be-used&#34;&gt;How do we take a machine learning model and put it out there to be used?&lt;/h5&gt;
&lt;h5 id=&#34;how-do-we-deploy-an-inference-model-out-there-in-the-real-world-to-interact-with-a-user&#34;&gt;How do we deploy an inference model out there in the real world, to interact with a user?&lt;/h5&gt;
&lt;p&gt;Amazon Web Services platform allows us to train, host and deploy a variety of models that we can use to gain insight from user data. For this instance, suppose we are looking to gauge opinions on a recent product, a film, a game, a trailer, etc.. Users might express their views via some form of text data (e.g. Tweets, reviews, Facebook posts, reddit comments). A deployed sentiment analysis model would be used to take in that data and infer the polarity of the data (positive or negative) and return to us, the number of positive and negative reviews respectively. This would be helpful to inform future product-related or marketing decisions regarding whatever product we&amp;rsquo;re putting out there.&lt;/p&gt;
&lt;p&gt;My foci for this project were a) developing my &lt;strong&gt;understanding of successful model deployment on AWS Sagemaker&lt;/strong&gt; and b) practicing writing a &lt;strong&gt;neural network model in pytorch&lt;/strong&gt; and the appropriate data preprocessing functions for it. Given that this is a widely-used, canonical dataset, model performance was less of a priority. To that end I&amp;rsquo;m including below a flow-chart of the architecture (Figure 1) - to follow the logic of what happens to a user&amp;rsquo;s inputted review, start from the top left (&amp;ldquo;User&amp;rsquo;s review&amp;rdquo;) and follow the arrows to the right.
I&amp;rsquo;ve included the training phase in the same diagram as deployment but it should be noted that training - shown through the righthand part of the diagram (labelled training data fed into the Sagemaker model and then stored in S3 buckets) - happens &lt;em&gt;prior&lt;/em&gt; to deployment.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;flow-chart&#34; srcset=&#34;
               /project/sentiment-analysis-modelling-with-pytorch/fig/sentiment_analysis_aws_flow_hu1d99e222f3b6a766d72cf462d1c736fd_128671_6be9b60775e3bb77510e7c528d4fbd3d.webp 400w,
               /project/sentiment-analysis-modelling-with-pytorch/fig/sentiment_analysis_aws_flow_hu1d99e222f3b6a766d72cf462d1c736fd_128671_74dd00a16cbf37242ef27e9739defb88.webp 760w,
               /project/sentiment-analysis-modelling-with-pytorch/fig/sentiment_analysis_aws_flow_hu1d99e222f3b6a766d72cf462d1c736fd_128671_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;/project/sentiment-analysis-modelling-with-pytorch/fig/sentiment_analysis_aws_flow_hu1d99e222f3b6a766d72cf462d1c736fd_128671_6be9b60775e3bb77510e7c528d4fbd3d.webp&#34;
               width=&#34;760&#34;
               height=&#34;580&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Fig 1 - Flow-chart showing how a Sagemaker notebook instance containing a Pytorch model interacts with an API, Lambda function and webapp&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Model performance and issues are supervised through AWS&amp;rsquo; CloudWatch platform, which provides a chronological log of activity from the model endpoint.&lt;/p&gt;
&lt;p&gt;A basic webapp, API and Lambda function were written for this project as well but are non-functioning as having these hosted on Sagemaker incurs significant costs. If you&amp;rsquo;re interested in replicating the project, the code here was tested while the S3 buckets and notebooks were still fully functioning so everything should run if cloned to a Sagemaker notebook instance perfectly, asuming all files are copied over. If you&amp;rsquo;re curious to see what the final, user-facing product would look like, there are some examples in Figures 2 and 3:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pos&#34; srcset=&#34;
               /project/sentiment-analysis-modelling-with-pytorch/fig/pos_huf342677c58750bdb400499c29a8920a2_112513_69b4e4246a866ba350815cb2312f2fda.webp 400w,
               /project/sentiment-analysis-modelling-with-pytorch/fig/pos_huf342677c58750bdb400499c29a8920a2_112513_287d5862b4078660187f115c76a4ee85.webp 760w,
               /project/sentiment-analysis-modelling-with-pytorch/fig/pos_huf342677c58750bdb400499c29a8920a2_112513_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;/project/sentiment-analysis-modelling-with-pytorch/fig/pos_huf342677c58750bdb400499c29a8920a2_112513_69b4e4246a866ba350815cb2312f2fda.webp&#34;
               width=&#34;760&#34;
               height=&#34;254&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Fig 2 - quite pleased that the model got past the sneakily positive adjectives.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;neg&#34; srcset=&#34;
               /project/sentiment-analysis-modelling-with-pytorch/fig/neg_hueaf939b9d0ded4bcadfadb59be3286f0_85586_46cff1ec5735d847a05bdb685b051a41.webp 400w,
               /project/sentiment-analysis-modelling-with-pytorch/fig/neg_hueaf939b9d0ded4bcadfadb59be3286f0_85586_d0b68549bfa5126ddb72a5c31198fa32.webp 760w,
               /project/sentiment-analysis-modelling-with-pytorch/fig/neg_hueaf939b9d0ded4bcadfadb59be3286f0_85586_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;/project/sentiment-analysis-modelling-with-pytorch/fig/neg_hueaf939b9d0ded4bcadfadb59be3286f0_85586_46cff1ec5735d847a05bdb685b051a41.webp&#34;
               width=&#34;760&#34;
               height=&#34;381&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Fig 3 - also pleased that this model seems to have taste&lt;/p&gt;
&lt;p&gt;Navigation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SageMaker Project.ipynb - main sagemaker notebook; contains more detailed and thorrough notes and comments on how the model is built and how the various components of AWS interact with each other&lt;/li&gt;
&lt;li&gt;website - directory containing webapp html code&lt;/li&gt;
&lt;li&gt;serve - directory with necessary deployment .py files:&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;model.py - model training functions&lt;/li&gt;
&lt;li&gt;predict.py - model deployment and inference functions&lt;/li&gt;
&lt;li&gt;utils.py - preprocessing functions for text data&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;train - directory with additional model training files:&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;requirements.txt - files needed by AWS for model to run&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;dict_data - directory containing word_dict.pkl - dictionary of unique integer to word key-value pairs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A significant amount of the material for this project was learned through Udacity&amp;rsquo;s Machine Learning Engineer Nanodegree. I am grateful for their content and resources and recommend the course.&lt;/p&gt;
&lt;p&gt;References:
Data -
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).
Pytorch - &lt;a href=&#34;https://pytorch.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pytorch.org/&lt;/a&gt;
If you&amp;rsquo;re interested in learning how to use Pytorch, they provide a variety of great tutorials at &lt;a href=&#34;https://pytorch.org/tutorials/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pytorch.org/tutorials/&lt;/a&gt;
Udacity - &lt;a href=&#34;https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009t&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Latent Semantic Analysis: intuition, math, implementation</title>
      <link>/publication/lsa_medium/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/publication/lsa_medium/</guid>
      <description>&lt;p&gt;=========================================================&lt;/p&gt;
&lt;h2 id=&#34;how-do-we-extract-themes-and-topic-from-text-using-unsupervised-learning&#34;&gt;How do we extract themes and topic from text using unsupervised learning&lt;/h2&gt;
&lt;hr&gt;
&lt;p&gt;TL;DR — Text data suffers heavily from high-dimensionality. Latent Semantic Analysis (LSA) is a popular, dimensionality-reduction techniques that follows the same method as Singular Value Decomposition. LSA ultimately reformulates text data in terms of &lt;em&gt;r&lt;/em&gt; &lt;strong&gt;latent&lt;/strong&gt;  (i.e. &lt;strong&gt;hidden&lt;/strong&gt;) features, where &lt;em&gt;r&lt;/em&gt; is less than &lt;em&gt;m&lt;/em&gt;, the number of terms in the data. I’ll explain the &lt;strong&gt;conceptual&lt;/strong&gt; and &lt;strong&gt;mathematical&lt;/strong&gt; intuition  and run a basic &lt;strong&gt;implementation&lt;/strong&gt; in &lt;a href=&#34;https://scikit-learn.org/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt; using the &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;20 newsgroups&lt;/a&gt; dataset.&lt;/p&gt;
&lt;p&gt;Language is more than the collection of words in front of you. When you read a text your mind conjures up images and notions. When you read many texts, themes begin to emerge, even if they’re never stated explicitly. Our innate ability to understand and process language defies an algorithmic expression (for the moment). LSA is one of the most popular Natural Language Processing (NLP) techniques for trying to determine themes within text mathematically. LSA is an unsupervised learning technique that rests on two pillars:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The distributional hypothesis, which states that words with similar meanings appear frequently together. This is best summarised by JR Firth’s quote “You shall know a word by the company it keeps” [1, p106]&lt;/li&gt;
&lt;li&gt;Singular Value Decomposition (SVD — Figure 1) a mathematical technique that we’ll be looking at in greater depth.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that LSA is an &lt;em&gt;unsupervised&lt;/em&gt; learning technique — there is no ground truth. The latent concepts might or might not be there! In the dataset we’ll use later we know there are 20 news categories and we can perform classification on them, but that’s only for illustrative purposes. It’ll often be the case that we’ll use LSA on unstructured, unlabelled data.&lt;/p&gt;
&lt;p&gt;Like all Machine Learning concepts, LSA can be broken down into 3 parts: the intuition, the maths and the code. Feel free to use the links in Contents to skip to the part most relevant to you. The full code is available in this &lt;a href=&#34;https://github.com/Ioana-P/pca_and_clustering_for_edu_purposes/blob/master/newsgroups_LSA.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A note on terminology: generally when decomposition of this kind is done on text data, the terms SVD and LSA (or LSI) are used interchangeably. From now on I’ll be using LSA, for simplicity’s sake.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article assumes some understanding of basic NLP preprocessing and of word vectorisation (specifically&lt;/em&gt; &lt;a href=&#34;https://medium.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;tf-idf vectorisation&lt;/em&gt;&lt;/a&gt;&lt;em&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;contents&#34;&gt;Contents:&lt;/h3&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#0c85&#34;&gt;Intuition&lt;/a&gt;: explanation with political news topics&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#d570&#34;&gt;The Math&lt;/a&gt;: SVD as a weighted, ordered sum of matrices &lt;strong&gt;or&lt;/strong&gt; as a set of 3 linear transformations&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#04db&#34;&gt;The code implementation&lt;/a&gt;: in Python3 with Scikit-Learn and 20Newsgroups data&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a131&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;1-intuition&#34;&gt;1. Intuition&lt;/h1&gt;
&lt;p&gt;(&lt;a href=&#34;#3ce3&#34;&gt;return to Contents&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In simple terms: LSA takes meaningful text documents and recreates them in &lt;em&gt;n&lt;/em&gt; different parts where each part expresses a different way of looking at meaning in the text. If you imagine the text data as a an idea, there would be &lt;em&gt;n&lt;/em&gt; different ways of &lt;em&gt;looking&lt;/em&gt; at that idea, or &lt;em&gt;n&lt;/em&gt; different ways of &lt;em&gt;conceptualising&lt;/em&gt; the whole text. LSA reduces our table of data to a table of latent (hidden_)_ concepts.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*1Sldip6QA_xwyyw7DI6SWw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 1: formula and matrix dimensions for SVD&lt;/p&gt;
&lt;p&gt;Suppose that we have some table of data, in this case text data, where each row is one document, and each column represents a term (which can be a word or a group of words, like “baker’s dozen” or “Downing Street”). This is the standard way to represent text data (in a &lt;em&gt;document-term matrix&lt;/em&gt;, as shown in Figure 2). The numbers in the table reflect how important that word is in the document. If the number is zero then that word simply doesn’t appear in that document.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*LuETpJGCTOaKAc8zyfjs4g.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 2: Document Term matrix, after applying some sort of vectorisation, in our case TF-IDF (but Bag of Words would also do)&lt;/p&gt;
&lt;p&gt;Different documents will be about different topics. Let’s say all the documents are &lt;strong&gt;politics&lt;/strong&gt; articles and there are 3 topics: &lt;strong&gt;foreign policy (F.P.), elections and reform&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*uPHKa66FY0XBnpMM2Kx-1w.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 3: Document-Topic matrix (or Document- &lt;strong&gt;Latent&lt;/strong&gt;-Concept if you prefer)&lt;/p&gt;
&lt;p&gt;Let’s say that there are articles strongly belonging to each category, some that are in two and some that belong to all 3 categories. We could plot a table where each row is a different document (a news article) and each column is a different topic. In the cells we would have a different numbers that indicated how strongly that document belonged to the particular topic (see Figure 3).&lt;/p&gt;
&lt;p&gt;Now if we shift our attention conceptually to the &lt;strong&gt;topics&lt;/strong&gt; themselves, we should ask ourselves the following question: &lt;em&gt;do we expect certain&lt;/em&gt; &lt;strong&gt;&lt;em&gt;words&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;to turn up more often in either of these topics?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we’re looking at foreign policy, we might see terms like “Middle East”, “EU”, “embassies”. For elections it might be “ballot”, “candidates”, “party”; and for reform we might see “bill”, “amendment” or “corruption”. So, if we plotted these topics and these terms in a different table, where the rows are the terms, we would see scores plotted for each term according to which topic it most strongly belonged. Naturally there will be terms that feature in all three documents (“prime minister”, “Parliament”, “decision”) and these terms will have scores across all 3 columns that reflect how much they belong to either category — the higher the number, the greater its affiliation to that topic. So, our second table (Figure 4) consists of terms and topics.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*b2T1vn1LLGWbCjat4tolGg.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 4: Term — Topic matrix&lt;/p&gt;
&lt;p&gt;Now the last component is a bit trickier to explain as a table. It’s actually a set of numbers, one for each of our topics. What do the numbers represent? They represent how much each of the topics &lt;em&gt;explains&lt;/em&gt; our data.&lt;/p&gt;
&lt;p&gt;How do they “explain” the data? Well, suppose that actually, “reform” wasn’t really a salient topic across our articles, and the majority of the articles fit in far more comfortably in the “foreign policy” and “elections”. Thus “reform” would get a really low number in this set, lower than the other two. An alternative is that maybe all three numbers are actually quite low and we actually should have had four or more topics — we find out later that a lot of our articles were actually concerned with economics! By sticking to just three topics we’ve been denying ourselves the chance to get a more detailed and precise look at our data. The technical name for this array of numbers is the “singular values”.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*vQY75Vct3QJJoPQlDLisKg.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 5: Singular values — what is the relative importance of our topics within our text?&lt;/p&gt;
&lt;p&gt;So that’s the intuition so far. You’ll notice that our two tables have one thing in common (the documents / articles) and all three of them have one thing in common — the topics, or some representation of them.&lt;/p&gt;
&lt;p&gt;Now let’s explain how this is a dimensionality reduction technique. It’s easier to see the merits if we specify a number of documents and topics. Suppose we had 100 articles and 10,000 different terms (just think of how many unique words there would be all those articles, from “amendment” to “zealous”!). In our original document-term matrix that’s 100 rows and 10,000 columns. When we start to break our data down into the 3 components, we can actually choose the number of topics — we could choose to have 10,000 different topics, if we genuinely thought that was reasonable. However, we could probably represent the data with far fewer topics, let’s say the 3 we originally talked about. That means that in our document-topic table, we’d slash about &lt;em&gt;99,997 columns&lt;/em&gt;, and in our term-topic table, we’d do the same. The columns and rows we’re discarding from our tables are shown as hashed rectangles in Figure 6. M  is the original document-term table; &lt;em&gt;U&lt;/em&gt; is the document-topic table, 𝚺 (sigma) is the array of singular values and &lt;em&gt;V-transpose&lt;/em&gt; (the superscript T means that the original matrix T has been flipped along its diagonal) is the document-topic table, but flipped on its diagonal (I’ll explain why in the math section).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*5najHCdleqnOpvZgJHB1kA.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 6 — what’s hashed we discard&lt;/p&gt;
&lt;p&gt;As for the set of numbers denoting topic importance, from a set of 10,000 numbers, each number getting smaller and smaller as it corresponds to a less important topic, we cut down to only 3 numbers, for our 3 remaining topics. This is why the Python implementation for LSA is called &lt;em&gt;Truncated&lt;/em&gt; SVD by the way: we’re cutting off part of our table, but we’ll get to the code later. It’s also worth noting that we don’t know what the 3 topics are in advance, we merely hypothesised that there would be 3 and, once we’ve gotten our components, we can explore them and see what the terms are.&lt;/p&gt;
&lt;p&gt;Of course, we don’t just want to return to the original dataset: we now have 3 lower-dimensional components we can use. In the code and maths parts we’ll go through which one we actually take forward. In brief, once we’ve truncated the tables (matrices), the product we’ll be getting out is the document-topic table (&lt;em&gt;U&lt;/em&gt;) &lt;em&gt;times&lt;/em&gt; the singular values (𝚺). This can be interpreted as the documents (all our news articles) along with how much they belong to each topic then &lt;strong&gt;weighted&lt;/strong&gt; by the relative importance of each topic. You’ll notice that in that case something’s been left out of this final table — the &lt;em&gt;words.&lt;/em&gt; Yes, we’ve gone beyond the words, we’re discarding them but keeping &lt;em&gt;the themes&lt;/em&gt;, which is a much more compact way to express our text.&lt;/p&gt;
&lt;h1 id=&#34;2-the-math&#34;&gt;2. The Math&lt;/h1&gt;
&lt;p&gt;(&lt;a href=&#34;#3ce3&#34;&gt;return to Contents&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;For the maths, I’ll be going through two different interpretations of SVD: first the general geometric decomposition that you can use with a real square matrix M and second the separable-models decomposition which is more pertinent to our example. SVD is also used in model-based recommendation systems. It is very similar to Principal Component Analysis (PCA), but it operates better on sparse data than PCA does (and text data is almost always sparse). Whereas PCA performs decomposition on the &lt;em&gt;correlation&lt;/em&gt; matrix of a dataset, SVD/LSA performs decomposition directly on the dataset as it is.&lt;/p&gt;
&lt;p&gt;We will be &lt;strong&gt;factorising&lt;/strong&gt; this matrix into constituent matrices. When I say factorising this is essentially the same as when we’re taking a number and representing it its factors, which when multiplied together, give us the original number, e.g. A = B * C * D .&lt;/p&gt;
&lt;p&gt;This is also why it’s called Singular Value &lt;strong&gt;Decomposition&lt;/strong&gt; — we’re &lt;em&gt;decomposing&lt;/em&gt; it into its constituent parts.&lt;/p&gt;
&lt;h2 id=&#34;general-geometric-decomposition&#34;&gt;General geometric decomposition&lt;/h2&gt;
&lt;p&gt;The extra dimension that wasn’t available to us in our original matrix, the &lt;em&gt;r&lt;/em&gt; dimension, is the amount of &lt;em&gt;latent concepts&lt;/em&gt;. Generally we’re trying to represent our matrix as other matrices that have one of their axes being this set of components. You will also note that, based on dimensions, the multiplication of the 3 matrices (when V is transposed) will lead us back to the shape of our original matrix, the &lt;em&gt;r&lt;/em&gt; dimension effectively disappearing.&lt;/p&gt;
&lt;p&gt;What matters in understanding the math is not the algebraic algorithm by which each number in U, V and 𝚺 is determined, but the mathematical properties of these products and how they relate to each other.&lt;/p&gt;
&lt;p&gt;First of all, it’s important to consider first what a matrix actually is and what it can be thought of — a transformation of vector space. In the top left corner of Figure 7 we have two perpendicular vectors. If we have only two variables to start with then the feature space (the data that we’re looking at) can be plotted anywhere in this space that is described by these two &lt;strong&gt;basis&lt;/strong&gt; vectors. Now moving to the right in our diagram, the matrix M is applied to this vector space and this transforms it into the new, transformed space in our top right corner. In the diagram below the geometric effect of M would be referred to as “shearing” the vector space; the two vectors &lt;em&gt;𝝈1&lt;/em&gt; and &lt;em&gt;𝝈2&lt;/em&gt; are actually our singular values plotted in this space.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*wGflVq-hpWnmUto3thkR6g.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 7: Source: Wikipedia; &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Singular Value Decomposition&lt;/a&gt;; &lt;a href=&#34;https://commons.wikimedia.org/wiki/File:Singular-Value-Decomposition.svg#filelinks;&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;; Author : Georg-Johann&lt;/p&gt;
&lt;p&gt;Now, just like with geometric transformations of points that you may remember from school, we can reconsider this transformation &lt;em&gt;M&lt;/em&gt; as three separate transformations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The rotation (or reflection) caused by &lt;em&gt;V*.&lt;/em&gt; Note that &lt;em&gt;V* = V-transpose&lt;/em&gt; as V is a real unitary matrix, so the complex conjugate of V is the same as its transpose. In vector terms, the transformation by V or &lt;em&gt;V*&lt;/em&gt; keeps the length of the basis vectors the same;&lt;/li&gt;
&lt;li&gt;𝚺 has the effect of stretching or compressing all coordinate points along the values of its singular values. Imagine our disc in the bottom left corner as we squeeze it vertically down in the direction of &lt;em&gt;𝝈2&lt;/em&gt; and stretch it horizontally along the direction of &lt;em&gt;𝝈1&lt;/em&gt;. These two singular values now can be pictured as the major and minor semi-axes of an ellipse. You can of course generalise this to &lt;em&gt;n&lt;/em&gt;-dimensions.&lt;/li&gt;
&lt;li&gt;Lastly, applying &lt;em&gt;U&lt;/em&gt; rotates (or reflects) our feature space. We’ve arrived at the same output as a transformation directly from &lt;em&gt;M&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I also recommend the excellent &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia entry on SVD&lt;/a&gt; as it has a particularly good explanation and GIF of the process.&lt;/p&gt;
&lt;p&gt;So, in other words, where &lt;em&gt;x&lt;/em&gt; is any column vector:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/lsa_medium/formula_1_huae8fae504900f8ae279e5e034c9661e9_15651_23dd397b521c2f9d1b44f82cff7c0d41.webp 400w,
               /publication/lsa_medium/formula_1_huae8fae504900f8ae279e5e034c9661e9_15651_08393934825db899d7d9e57c5883571c.webp 760w,
               /publication/lsa_medium/formula_1_huae8fae504900f8ae279e5e034c9661e9_15651_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;/publication/lsa_medium/formula_1_huae8fae504900f8ae279e5e034c9661e9_15651_23dd397b521c2f9d1b44f82cff7c0d41.webp&#34;
               width=&#34;520&#34;
               height=&#34;249&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The transformation by M on x is the same as the three transformations on x by the matrices on the right&lt;/p&gt;
&lt;p&gt;One of the properties of the matrices &lt;em&gt;U&lt;/em&gt; and &lt;em&gt;V*&lt;/em&gt; is that they’re unitary, so we can say that the columns of both of these matrices form two sets of orthonormal basis vectors. In other words, the column vectors you can get from U would form their own coordinate space, such that if there were two columns &lt;em&gt;U1&lt;/em&gt; and &lt;em&gt;U2,&lt;/em&gt; you could write out all of the coordinates of the space as combinations of &lt;em&gt;U1&lt;/em&gt; and &lt;em&gt;U2&lt;/em&gt;. The same applies to the columns of &lt;em&gt;V&lt;/em&gt;, &lt;em&gt;V1&lt;/em&gt; and &lt;em&gt;V2,&lt;/em&gt; and this would generalise to &lt;em&gt;n&lt;/em&gt;-dimensions (you’d have &lt;em&gt;n&lt;/em&gt;-columns).&lt;/p&gt;
&lt;h1 id=&#34;separable-models-decomposition&#34;&gt;Separable models decomposition&lt;/h1&gt;
&lt;p&gt;We can arrive at the same understanding of PCA if we imagine that our matrix M can be broken down into a weighted sum of separable matrices, as shown below.&lt;/p&gt;
&lt;p&gt;Decomposition of our data M into a weighted sum of separable matrices, &lt;em&gt;Ai&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The matrices 𝐴𝑖 are said to be separable because they can be decomposed into the outer product of two vectors, weighted by the singular value 𝝈_i_. Calculating the outer product of two vectors with shapes (&lt;em&gt;m,&lt;/em&gt;) and (&lt;em&gt;n,&lt;/em&gt;) would give us a matrix with a shape (m,n). In other words, every possible product of any two numbers in the two vectors is computed and placed in the new matrix. The singular value not only weights the sum but orders it, since the values are arranged in descending order, so that the first singular value is always the highest one.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*D8WG84Sg8zkOZl6olTK8ng.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 8: our separable matrices. Note the ≅ sign is representing the fact that the decomposed set of 3 products only &lt;strong&gt;approximates&lt;/strong&gt; our original matrix, it does not equal it exactly.&lt;/p&gt;
&lt;p&gt;In Figure 8 you can see how you could visualise this. Previously we had the tall &lt;em&gt;U&lt;/em&gt;, the square &lt;em&gt;Σ&lt;/em&gt; and the long 𝑉-&lt;em&gt;transpose&lt;/em&gt; matrices. Now you can picture taking the first vertical slice from &lt;em&gt;U&lt;/em&gt;, weighting (multiplying) all its values by the first singular value and then, by doing an outer product with the first horizontal slice of 𝑉_-transpose_, creating a new matrix with the dimensions of those slices. Then we add those products together and we get &lt;em&gt;M&lt;/em&gt;. Or, if we don’t do the full sum but only complete it partially, we get the truncated version.&lt;/p&gt;
&lt;p&gt;So, for our data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where &lt;em&gt;M&lt;/em&gt; is our original (&lt;em&gt;m, n&lt;/em&gt;) data matrix — m rows, n columns; &lt;em&gt;m documents, n terms&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;U is a (&lt;em&gt;m, r&lt;/em&gt;) matrix — &lt;em&gt;m documents and r concepts&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Σ is a &lt;em&gt;diagonal&lt;/em&gt; (&lt;em&gt;r , r&lt;/em&gt;) matrix — all values except those in the diagonal are zero. (But what do the non-zero values represent?&lt;/li&gt;
&lt;li&gt;V is a (&lt;em&gt;n, r&lt;/em&gt;) matrix — &lt;em&gt;n terms, r concepts&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The values in 𝚺 represent how much each latent concept explains the variance in our data. When these are multiplied by the &lt;em&gt;u&lt;/em&gt; column vector for that latent concept, it will effectively weigh that vector.&lt;/p&gt;
&lt;p&gt;If we were to decompose this to 5 components, this would look something like this:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /publication/lsa_medium/formula_2_hu3038c161a4454db15d2e72400bed9bc7_14673_087a12892503fbcada5b088d5d27d77e.webp 400w,
               /publication/lsa_medium/formula_2_hu3038c161a4454db15d2e72400bed9bc7_14673_1d2507fdbdd2dad7307882d91dbefed5.webp 760w,
               /publication/lsa_medium/formula_2_hu3038c161a4454db15d2e72400bed9bc7_14673_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;/publication/lsa_medium/formula_2_hu3038c161a4454db15d2e72400bed9bc7_14673_087a12892503fbcada5b088d5d27d77e.webp&#34;
               width=&#34;535&#34;
               height=&#34;196&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A sum of the outer product of our weighted document-concept vector and our term-concept vector&lt;/p&gt;
&lt;p&gt;where there would be originally &lt;em&gt;r&lt;/em&gt; number of &lt;em&gt;u&lt;/em&gt; vectors; 5 singular values and n number of 𝑣_-transpose_ vectors.&lt;/p&gt;
&lt;h1 id=&#34;3-the-code-implementation&#34;&gt;3. The code implementation&lt;/h1&gt;
&lt;p&gt;(&lt;a href=&#34;#3ce3&#34;&gt;return to Contents&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In this last section we’ll see how we can implement basic LSA using Scikit-Learn.&lt;/p&gt;
&lt;h2 id=&#34;extract-transform-and-load-our-text-data&#34;&gt;Extract, Transform and Load our text data&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn.datasets import fetch\_20newsgroups  
X\_train, y\_train = fetch\_20newsgroups(subset=&amp;#39;train&amp;#39;, return\_X\_y=True)  
X\_test, y\_test = fetch\_20newsgroups(subset=&amp;#39;test&amp;#39;, return\_X\_y=True)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;cleaning-and-preprocessing&#34;&gt;Cleaning and Preprocessing&lt;/h2&gt;
&lt;p&gt;The cleaning of text data is often a very different beast from cleaning of numerical data. You’ll often find yourself having prepared your vectoriser, you model and you’re ready to Gridsearch and then extract features, only to find that the most important features in cluster &lt;em&gt;x&lt;/em&gt; is the string “___” … so you go back…and do more cleaning. The code block below came about as a result of me realizing that I needed to remove website URLs, numbers and emails from the dataset.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from nltk.corpus import stopwords  
from nltk.tokenize import RegexpTokenizer  
import re  
tokenizer = RegexpTokenizer(r&amp;#39;\\b\\w{3,}\\b&amp;#39;)  
stop\_words = list(set(stopwords.words(&amp;#34;english&amp;#34;)))  
stop\_words += list(string.punctuation)  
stop\_words += \[&amp;#39;\_\_&amp;#39;, &amp;#39;\_\_\_&amp;#39;\]\# Uncomment and run the 3 lines below if you haven&amp;#39;t got these packages already  
\# nltk.download(&amp;#39;stopwords&amp;#39;)  
\# nltk.download(&amp;#39;punkt&amp;#39;)  
\# nltk.download(&amp;#39;wordnet&amp;#39;)def rmv\_emails\_websites(string):  
    &amp;#34;&amp;#34;&amp;#34;Function removes emails, websites and numbers&amp;#34;&amp;#34;&amp;#34; new\_str = re.sub(r&amp;#34;\\S+@\\S+&amp;#34;, &amp;#39;&amp;#39;, string)  
    new\_str = re.sub(r&amp;#34;\\S+.co\\S+&amp;#34;, &amp;#39;&amp;#39;, new\_str)  
    new\_str = re.sub(r&amp;#34;\\S+.ed\\S+&amp;#34;, &amp;#39;&amp;#39;, new\_str)  
    new\_str = re.sub(r&amp;#34;\[0-9\]+&amp;#34;, &amp;#39;&amp;#39;, new\_str)  
    return new\_strX\_train = list(map(rmv\_emails\_websites, X\_train))  
X\_test  = list(map(rmv\_emails\_websites, X\_test))
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;tokenising-and-vectorising-text-data&#34;&gt;Tokenising and vectorising text data&lt;/h2&gt;
&lt;p&gt;Our models work on numbers, not string! So we tokenise the text (turning all documents into smaller observational entities — in this case words) and then turn them into numbers using Sklearn’s TF-IDF vectoriser. I recommend with any transformation process (especially ones that take time to run) you do them on the first 10 rows of your data and inspect results: are they what you expected to see? Is the shape of the dataframe what you hoped for? Once you’re feeling confident of your code, feed in the whole corpus.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;tfidf = TfidfVectorizer(lowercase=True,   
                        stop\_words=stop\_words,   
                        tokenizer=tokenizer.tokenize,   
                        max\_df=0.2,  
                        min\_df=0.02  
                       )  
tfidf\_train\_sparse = tfidf.fit\_transform(X\_train)  
tfidf\_train\_df = pd.DataFrame(tfidf\_train\_sparse.toarray(),   
                        columns=tfidf.get\_feature\_names())  
tfidf\_train\_df.head()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This should give you your vectorised text data — the document-term matrix. Repeat the steps above for the test set as well, but &lt;strong&gt;only&lt;/strong&gt; using transform, &lt;strong&gt;not&lt;/strong&gt; fit_transform.&lt;/p&gt;
&lt;h2 id=&#34;lsa-for-exploratory-data-analysis-eda&#34;&gt;LSA for Exploratory Data Analysis (EDA)&lt;/h2&gt;
&lt;p&gt;Just for the purpose of visualisation and EDA of our decomposed data, let’s fit our LSA object (which in Sklearn is the &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TruncatedSVD class&lt;/a&gt;) to our train data and specifying only 20 components.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from sklearn.decomposition import TruncatedSVDlsa\_obj = TruncatedSVD(n\_components=20, n\_iter=100, random\_state=42)tfidf\_lsa\_data = lsa\_obj.fit\_transform(tfidf\_train\_df)  
Sigma = lsa\_obj.singular\_values\_  
V\_T = lsa\_obj.components\_.T
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now let’s visualise the singular values — is the barplot below showing us what we expected of them?&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sns.barplot(x=list(range(len(Sigma))), y = Sigma)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/640/1*U6GCUrfJ1hfOwI7fBkzwNw.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 9 — our singular values, representing how much each latent concept &lt;em&gt;explains the variance in the data&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let’s explore our reduced data through the term-topic matrix, &lt;em&gt;V-tranpose.&lt;/em&gt; TruncatedSVD will return it to as a numpy array of shape (num_documents, num_components), so we’ll turn it into a Pandas dataframe for ease of manipulation.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;term\_topic\_matrix = pd.DataFrame(data=lsa\_term\_topic,   
                                 index = eda\_train.columns,   
                                 columns = \[f&amp;#39;Latent\_concept\_{r}&amp;#39; for r in range(0,V\_T.shape\[1\])\])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Let’s slice our term-topic matrix into Pandas Series (single column data-frames), sort them by value and plot them. The code below plots this for our 2nd latent component (recall that in python we start counting from 0) and returns the plot in Figure 10:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;data = term\_topic\_matrix\[f&amp;#39;Latent\_concept\_1&amp;#39;\]  
data = data.sort\_values(ascending=False)  
top\_10 = data\[:10\]  
plt.title(&amp;#39;Top terms along the axis of Latent concept 1&amp;#39;)  
fig = sns.barplot(x= top\_10.values, y=top\_10.index)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/640/1*eBXrjYQF7Vao3pLSLGEHiA.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 10: despite the seeming noise at least 3 terms here have a strong theme&lt;/p&gt;
&lt;p&gt;These are the words that rank highly along our 2nd latent component. What about the words at the other end of this axis (see Fig 11)?&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/640/1*2E2kZjCp4hUk-mVTrKj_-Q.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 11: it was at this moment the author appreciated how useful lemming/stemming would’ve been&lt;/p&gt;
&lt;p&gt;You can make your own mind up about that this semantic divergence signifies. Adding more preprocessing steps would help us cleave through the noise that words like “say” and “said” are creating, but we’ll press on for now. Let’s do one more pair of visualisations for the 6th latent concept (Figures 12 and 13).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/640/1*sH66WI7jpF5eTiaLhUylgQ.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 12
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/640/1*J8wVB1n2hcrREesFjhxu_Q.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 13: we see once again that technological terms feature strongly in this data&lt;/p&gt;
&lt;p&gt;At this point it’s up to us to infer some meaning from these plots. The negative end of concept 5’s axis seems to correlate very strongly with technological and scientific themes (‘space’, ‘science’, ‘computer’), but so does the positive end, albeit more focused on computer related terms (‘hard’, ‘drive’, ‘system’).&lt;/p&gt;
&lt;p&gt;Now just to be clear, determining the right amount of components will require tuning, so I didn’t leave the argument set to 20, but changed it to 100. You might think that’s still a large number of dimensions, but our original was 220 (and that was with constraints on our minimum document frequency!), so we’ve reduced a sizeable chunk of the data. I’ll explore in another post how to choose the optimal number of singular values. For now we’ll just go forward with what we have.&lt;/p&gt;
&lt;h2 id=&#34;using-our-latent-components-in-our-modelling-task&#34;&gt;Using our latent components in our modelling task&lt;/h2&gt;
&lt;p&gt;Although LSA is an unsupervised technique often used to find patterns in unlabelled data, we’re using it here to reduce the dimensions of labelled data before feeing it into a model. We’ll compare our accuracy on the LSA data with the accuracy on our standard TF-IDF data to gauge how much useful information the LSA has captured from the original dataset. We now have a train dataset of shape (11314, 100). The number of documents is preserved and we have created 100 latent concepts. Now let’s run a model on this and on our standard TF-IDF data. The aim of the implementation below isn’t to get a great model, but to compare the two very different datasets. I’ve included basic cross validation through GridSearchCV and performed a tiny amount of tuning for the tolerance hyperparameter. If you were to do this for the sake of building an actual model, you would go much farther than what’s written below. This is just to help you get a basic implementation going:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;logreg\_lsa = LogisticRegression()  
logreg     = LogisticRegression()  
logreg\_param\_grid = \[{&amp;#39;penalty&amp;#39;:\[&amp;#39;l1&amp;#39;, &amp;#39;l2&amp;#39;\]},  
                 {&amp;#39;tol&amp;#39;:\[0.0001, 0.0005, 0.001\]}\]grid\_lsa\_log = GridSearchCV(estimator=logreg\_lsa,  
                        param\_grid=logreg\_param\_grid,   
                        scoring=&amp;#39;accuracy&amp;#39;, cv=5,  
                        n\_jobs=-1)grid\_log = GridSearchCV(estimator=logreg,  
                        param\_grid=logreg\_param\_grid,   
                        scoring=&amp;#39;accuracy&amp;#39;, cv=5,  
                        n\_jobs=-1)best\_lsa\_logreg = grid\_lsa\_log.fit(tfidf\_lsa\_data, y\_train).best\_estimator\_  
best\_reg\_logreg = grid\_log.fit(tfidf\_train\_df, y\_train).best\_estimator\_print(&amp;#34;Accuracy of Logistic Regression on LSA train data is :&amp;#34;, best\_lsa\_logreg.score(tfidf\_lsa\_data, y\_train))  
print(&amp;#34;Accuracy of Logistic Regression with standard train data is :&amp;#34;, best\_reg\_logreg.score(tfidf\_train\_df, y\_train))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Which returns:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Accuracy of Logistic Regression on LSA train data is : 0.45  
Accuracy of Logistic Regression with standard train data is : 0.52
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The drop in performance is significant, but you can work this into an optimisation pipeline and tweak the number of latent components. How does this perform on our test data (7532 documents) though?&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Accuracy of Logistic Regression on LSA test data is : 0.35  
Accuracy of Logistic Regression on standard test data is : 0.37
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Accuracy has dropped greatly for both, but notice how small the gap between the models is! Our LSA model is able to capture about as much information from our test data as our standard model did, with less than half the dimensions! Since this is a multi-label classification it would be best to visualise this with a confusion matrix (Figure 14). Our results look significantly better when you consider the random classification probability given 20 news categories. If you’re not familiar with a confusion matrix, as a rule of thumb, we want to maximise the numbers down the diagonal and minimise them everywhere else.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://miro.medium.com/max/720/1*I24_6q3dvnKNuLSw2Sth0Q.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 14— &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Confusion matrix&lt;/a&gt; for our test data (7532 documents). y-axis represents actual news categories, x-axis represents predicted news categories. The diagonal values are all the correctly classified documents.&lt;/p&gt;
&lt;p&gt;And that concludes our implementation of LSA in Scikit-Learn. We’ve covered the intuition, mathematics and coding of this technique.&lt;/p&gt;
&lt;p&gt;I hope you’ve enjoyed this post and would appreciate any amount of claps. Feel free to leave any feedback (positive or constructive) in the comments, especially about the math section, since I found that the most challenging to articulate.&lt;/p&gt;
&lt;h1 id=&#34;4-references&#34;&gt;4. References&lt;/h1&gt;
&lt;p&gt;(&lt;a href=&#34;http://3ce3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;return to Contents&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;p&gt;[1] L. Hobson, H. Cole, H. Hapke, &lt;a href=&#34;https://www.manning.com/books/natural-language-processing-in-action&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Natural Language Processing in Action&lt;/a&gt; (2019), &lt;a href=&#34;https://www.manning.com/books/natural-language-processing-in-action&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.manning.com/books/natural-language-processing-in-action&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Pedregosa &lt;em&gt;et al.,&lt;/em&gt; &lt;a href=&#34;http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-learn: Machine Learning in Python&lt;/a&gt; (2011), JMLR 12, pp. 2825–2830.&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&#34;https://towardsdatascience.com/@yassine.hamdaoui?source=post_page-----6c2b61b78558----------------------&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hamdaoui&lt;/a&gt; Y, &lt;a href=&#34;https://medium.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python&lt;/a&gt; (2019), Towards Data Science&lt;/p&gt;
&lt;p&gt;[4] Wikipedia contributers, &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Singular Value Decomposition&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Singular_value_decomposition&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gently guiding the (machine) learner</title>
      <link>/publication/active_ml/</link>
      <pubDate>Thu, 02 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/publication/active_ml/</guid>
      <description>&lt;p&gt;====================================&lt;/p&gt;
&lt;h2 id=&#34;1-why-should-we-be-interested&#34;&gt;1. Why should we be interested?&lt;/h2&gt;
&lt;p&gt;Often in Data Science and Machine Learning we are constrained by a lack of adequate labelled data. We have the tried-and-tested Supervised Learning (SL) algorithms, we have an idea of what type of task and output we would like to see, we can even imagine that glorious insight we&amp;rsquo;d gain from applying our Neural Nets and Decision Trees, BUT we just don&amp;rsquo;t have the (labelled) data.&lt;/p&gt;
&lt;p&gt;Generating labelled data sets is time-consuming, expensive and ludicrously tedious. Imagine having to go through several hundred thousand photos and tag them with the number of pine trees they contain. You&amp;rsquo;d get bored very quickly. Now imagine if you had to read through hundreds of thousands of lines of text, labelling them with whether the sentence is in active or passive voice. On top of being bored, you&amp;rsquo;ll get sloppy at some point, so an additional labeller will probably be needed. The problem becomes worse for complex data points, such as long legal documents or health records. For any organisation trying to leverage its data, the cost of this (in terms of time, money and, frankly, morale!) will quickly ramp up! So instead of using SL, we can turn to a form of Semi-Supervised Learning, specifically, Active Machine Learning.
2. What is Active Machine Learning?
The purpose of Active ML is to supply the smallest necessary amount of labelled data to produce a robust learner while minimising human intervention. The human labelling is restricted to those cases where it has the maximum usefulness. Now some questions arise immediately from that statement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do you determine the &amp;ldquo;smallest necessary amount&amp;rdquo;?&lt;/li&gt;
&lt;li&gt;Which data should go into that amount?&lt;/li&gt;
&lt;li&gt;What are we defining as &amp;ldquo;usefulness&amp;rdquo;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;rsquo;ll delve deeper into those issues in section 3. Figure 1 shows the general steps in active ML. The green numbers represent our steps detailed below:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*WwBlS7WJ_C_VclDQXff4Og.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Fig 1: general architecture of active machine learning&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The human expert only labels a subset of that data (say 10%; the individual steps are shown in green). This cuts down massively on labelling time. Let&amp;rsquo;s refer to this initial labelled data as the seed data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We train our machine learner (which could be ANY type of algorithm suited to our task, be it regressor or classifier) JUST on the seed data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The now trained learner generates predictions on the rest of the data set and provides a value of how confident it is for each of its predictions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The learner returns the predictions with the lowest confidence ratings to the human domain expert (kind of like a student going to a teacher with their homework saying &amp;ldquo;I wasn&amp;rsquo;t really sure how to do these…&amp;rdquo;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The human expert only labels these low-confidence data points.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally the additionally labelled data is fed into the labelled data set and, along with the seed data, is used to retrain our machine learner.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A lot of the problems arise in Step 1, where we select our seed data. Before addressing that, let&amp;rsquo;s go through an example.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*w14qLn3kiYD_I7KUq3Fu9w.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Fig 2: a boundary case; the learner hasn&amp;rsquo;t quite figured out the features that make a &amp;ldquo;4&amp;rdquo; a &amp;ldquo;4&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Imagine you&amp;rsquo;re training a classifier on the MNIST handwritten digits dataset (a collection of pixelated images of single digit numbers from 0 to 9). Suppose that we lost all the labels for this data. We could just the then you would train your learner on a subsection of the data that was labelled. It is useful to think of the low-confidence data points as being highly discriminant - they are very good boundary cases that test your learner&amp;rsquo;s comprehension of the data. Then, when our learner makes predictions on the rest of the data, it will return us the predictions with the lowest confidence, as shown in&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*w14qLn3kiYD_I7KUq3Fu9w.jpeg&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
Figure 2. A classifier might misidentify the number &amp;ldquo;4&amp;rdquo; as a &amp;ldquo;1&amp;rdquo;, but we can see why it would do so.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*yRkOQoLM2FpHL8vaJVb1RQ.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Figure 3: I went with &amp;ldquo;7&amp;rdquo; but honestly your guess is as good as mine&lt;/p&gt;
&lt;p&gt;In Figure 3 you can see an example of what this would look like in a Notebook (I used the standard example code for modAL, an Active Machine Learning library built to be compatible with Scikit-Learn).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;3. What are the limitations?
How do you determine the &amp;ldquo;smallest necessary amount&amp;rdquo;?
Which data should go into that amount?
What are we defining as &amp;ldquo;usefulness&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;This questions is domain- and data-specific. If you apply both standard supervised learning and AML to a dataset, as you increase the amount of initial labelled data supplied, your accuracy will increase in both cases. However, if the assumptions of AML are robust, then the accuracy curve will be steeper for the active learning process than for the standard supervised learning technique. In this case you should determine the relative costs of labelling more data and seeing what the uppermost limit on data labelling would be. There is no definitive answer (yet!) to this question, so empirically trying out the method on any previously labelled data sets to determine a cost-optimal threshold would be a good way to start.
This is an area of active research: ideally you need to provide an initial data set that is significantly representative of your total data set. Any bias in your initial selection will have significant repercussions in the testing stage, which is a considerable drawback of active learning. A pseudo-random selection from the data (as I have done with the MNIST dataset above) seems promising, but does not guarantee representativeness.&lt;/p&gt;
&lt;p&gt;How useful would this row of data be to our learner if it were labelled? Active learning involves the machine choosing which instances to send back to the human (teacher). There are a variety of ways our learner can be set up to choose the most useful data to have labelled, normally based on the learner&amp;rsquo;s confidence, e.g. least-confidence (querying about the instances which the learner is least confident of in its prediction); margin sampling (chooses instances where the classification margin is the narrowest); query-by-committee (a set of different models are trained on the data and the instances where the greatest disagreement occurs are deemed most useful).&lt;/p&gt;
&lt;p&gt;There is still a lot unknown in AML, most significantly the fact that we often don&amp;rsquo;t know in advance if a particular dataset or type will benefit from this semi-supervised approach. Nevertheless, the literature survey [1] suggests that there are some areas of significant success, especially within Natural Language Processing (one example of AML being used successfully in Named-Entity-Recognition here).&lt;/p&gt;
&lt;h2 id=&#34;4-references&#34;&gt;4. References&lt;/h2&gt;
&lt;p&gt;[1] Olsson, Fredrik (2009) A literature survey of active machine learning in the context of natural language processing. [SICS Report]. Available at: &lt;a href=&#34;http://eprints.sics.se/3600/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://eprints.sics.se/3600/&lt;/a&gt; [accessed on 30.12.2019]&lt;/p&gt;
&lt;p&gt;[2] Deep Learning Scaling is Predictable, Empirically; Hestness, J. et al, arXiv:1712.00409 [cs.LG].
Active (Machine) Learning - Computerphile. &lt;a href=&#34;https://www.youtube.com/watch?v=ANIw1Mz1SRI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=ANIw1Mz1SRI&lt;/a&gt;. [accessed on: 30.12.2019]&lt;/p&gt;
&lt;p&gt;[3] The code snippets were completed using modAL:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;@article{modAL2018,
    title={mod{AL}: {A} modular active learning framework for {P}ython},
    author={Tivadar Danka and Peter Horvath},
    url={https://github.com/cosmic-cortex/modAL},
    note={available on arXiv at \url{https://arxiv.org/abs/1805.00979}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analysing scraped job data</title>
      <link>/project/job-market-exploration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/project/job-market-exploration/</guid>
      <description>&lt;h1 id=&#34;exploring-the-uk-data-science-job-market-through-indeed-job-posts&#34;&gt;Exploring the UK Data Science job market through Indeed job posts&lt;/h1&gt;
&lt;p&gt;I scraped 1082 job posts from Indeed.co.uk, searching for 3 different types of data science roles via the title, and then analysed the data. Please bear in mind that all the data was off one site only (I hope to be able to get job data off LinkedIn/Glassdoor at some point in the future) in late Nov 2020.&lt;/p&gt;
&lt;p&gt;Bear in mind that this is an &lt;strong&gt;exploratory project&lt;/strong&gt; - findings should be taken with a pinch of salt, and really the results need to be corroborated with a ~~replication of the project in 2021 or with other research. ~~
Update 01/06/2022:
The scraping code has been updated again and tested on a small sample (n=10) of jobs with the search term title:(data,scientist).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Location is now taken directly from the search results page, which seems to be returning more locations than the alternative as it is a more easily identifiable element&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Retrieve&amp;rdquo; has been removed from the GUI options. This was originally intended for the purpose of retrieving historical data from an SQLite db, but I&amp;rsquo;ve mothballed that idea now since it didn&amp;rsquo;t give enough clear benefits.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;findings&#34;&gt;Findings&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Most Data Science jobs do not advertise their salary openly (only 41.5% do)&lt;/li&gt;
&lt;li&gt;The median Data Science salary is about £45k; the mean is about £51k&lt;/li&gt;
&lt;li&gt;The majority of roles are based in London&lt;/li&gt;
&lt;li&gt;The median Data Analyst role is under £40k, whereas the median Data Scientist and Machine Learning jobs are above £60k. Roles with &amp;ldquo;Scientist&amp;rdquo; and/or &amp;ldquo;Machine learning&amp;rdquo; earn £20k more (&lt;strong&gt;on average&lt;/strong&gt;) than roles with &amp;ldquo;Analyst&amp;rdquo; in the title.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/job-market-exploration/fig/salaries_all_hu4969e48710dd2b978bb8969624a5a666_60594_27646ab952fe057815f2a6bea464f4e6.webp 400w,
               /project/job-market-exploration/fig/salaries_all_hu4969e48710dd2b978bb8969624a5a666_60594_defd3dd196499dbe35ac06cadd96b836.webp 760w,
               /project/job-market-exploration/fig/salaries_all_hu4969e48710dd2b978bb8969624a5a666_60594_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;/project/job-market-exploration/fig/salaries_all_hu4969e48710dd2b978bb8969624a5a666_60594_27646ab952fe057815f2a6bea464f4e6.webp&#34;
               width=&#34;760&#34;
               height=&#34;691&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;London and Cambridge have the highest average salaries; the former has the widest spread of salaries&lt;/li&gt;
&lt;li&gt;If you&amp;rsquo;re a median-salary London Data Analyst, you&amp;rsquo;re getting paid as well as the median salary for all Data Science roles outside the company.&lt;/li&gt;
&lt;li&gt;Just over 14% of overall roles are looking for a &amp;ldquo;Senior&amp;rdquo; hire. Less than 2% explicitly advertise for a &amp;lsquo;Junior&amp;rsquo; hire.&lt;/li&gt;
&lt;li&gt;The most popular languages / skills (based on our search) were: Python, SQL, R, AWS and Azure (in decreasing order of mentions).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/job-market-exploration/fig/percentage_mentions_by_group_hu8d36c2c10dd8a064e7a3b8b5c94791ad_148193_7d786cb87c2eec60085c33d40a11c0ee.webp 400w,
               /project/job-market-exploration/fig/percentage_mentions_by_group_hu8d36c2c10dd8a064e7a3b8b5c94791ad_148193_c4bf678cc770ffdfe0583142c80b396b.webp 760w,
               /project/job-market-exploration/fig/percentage_mentions_by_group_hu8d36c2c10dd8a064e7a3b8b5c94791ad_148193_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;/project/job-market-exploration/fig/percentage_mentions_by_group_hu8d36c2c10dd8a064e7a3b8b5c94791ad_148193_7d786cb87c2eec60085c33d40a11c0ee.webp&#34;
               width=&#34;760&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;Using feature engineering and 3 different iterations of models, I was not able to predict salary from job description and title data. This indicates that either more data is needed (likely since there only 274 data points in the training data) or that it&amp;rsquo;s not possible to reliably predict the salary from such data.&lt;/li&gt;
&lt;li&gt;There was insufficient data to determine if years of experience required / requested correlated with annual salary.&lt;/li&gt;
&lt;li&gt;After using topic modelling, a few topics stood out - most noticeably the &lt;code&gt;Academic_&amp;amp;_Research&lt;/code&gt; topic which seems to be the only one that has a moderate correlation with salary.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Full method and implementation in notebook - Data_Scientist_UK_Q4_Job_market_analysis.ipynb&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;future-steps&#34;&gt;Future Steps&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Improve webscraping process to pick up &amp;ldquo;Remote work&amp;rdquo; tags that may be stored sepately on the job website to the main text&lt;/li&gt;
&lt;li&gt;Collect more data in 2021 and reiterate project to test current findings&lt;/li&gt;
&lt;li&gt;Using feature engineering and regex, for future batches, determine subgroup of jobs that are research-focussed and test again if those roles do correlate with salary.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;repo-navigation&#34;&gt;Repo navigation:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;index.ipynb&lt;/strong&gt; - principal notebook; questions and project plan located there.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;scrape_indeed_gui.py&lt;/strong&gt; - script for running Job Scraper Tool (built using pySimpleGui)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;archive/&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cleaning.ipynb - notebook to check the outputs of the scraping tool&amp;rsquo;s results&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;untitled.ipynb - nb used to load and check extraneous data (e.g. ONS salary information for sector)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;clean_data/ - folder preprocessed data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;raw_data/ folder including data as immediately outputed after webscraping stage&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA_vis_plot.html - interactive visualisation of Latent Dirichlet Allocation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;functions.py - scripted functions and classes stored here, including webscraping tool&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;topic_mod.py - functions for topic modelling&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fig/ - noteable visualisations saved here&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;references&#34;&gt;References:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SlashData Report - &amp;lsquo;State of the Developer Nation 19th Edition&amp;rsquo; - &lt;a href=&#34;https://slashdata-website-cms.s3.amazonaws.com/sample_reports/y7fzAZ8e5XuKCL1Q.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://slashdata-website-cms.s3.amazonaws.com/sample_reports/y7fzAZ8e5XuKCL1Q.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Logistic Ordinal Regression - &lt;a href=&#34;http://fa.bianp.net/blog/2013/logistic-ordinal-regression/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://fa.bianp.net/blog/2013/logistic-ordinal-regression/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pyLDAvis Overview - &lt;a href=&#34;https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-assumptions-to-bear-in-mind&#34;&gt;Key Assumptions to bear in mind:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data sourcing&lt;/strong&gt; - Data was sourced purely from Indeed.co.uk over a limited time span. The individual time of scraping is recorded for each job post inside the raw_data file raw_data/SE_jobs_20_11_2020.csv. Further sampling over time will be needed to replicate or falsify findings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;National averages&lt;/strong&gt; - ONS data was filtered to retrieve data for 2019 as 2020 data is a. not fully available yet broken down by and b. 2020 survey data was affected by the Covid19 lockdown and the move to telephone polling. Although the 2019 mean salary is not a fair comparison for data retrieved in Q3 2020, it provides a rough benchmark against which we can compare our sample.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data mining&lt;/strong&gt; - The salary extraction method is reliant on the pattern finding of text that &amp;ldquo;looks like salary data&amp;rdquo; - i.e. the python script that I wrote searched specifically for text that included &amp;ldquo;£&amp;rdquo; followed by any length of numbers (continuous or punctuated by a comma) and a time period phrase such as &amp;ldquo;per day&amp;rdquo;, &amp;ldquo;per annum&amp;rdquo;, &amp;ldquo;a year&amp;rdquo;, etc. Spot-checking showed the method to be robust. Where a range was stated (e.g. &amp;ldquo;£40,000 - £50,000&amp;rdquo;) the mean was taken. The same applied to regex mining for programming languages: particular challenges were encountered where languages had many variants (JavaScript for instance) or even different spelling (Javascript) - I tried to capture as many as feasible in the regex patterns. It might seem strange that the values for C++ and C# came out near zero, but, after multiple rounds of testing, I still could not find any, even though my function searching for those two languages had specific conditions written to ensure that they were detected.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Webscraping process&lt;/strong&gt; - The webscraping tool generally managed to retrieve the maximum of 19 jobs posted per page on Indeed&amp;rsquo;s search result pages. The first part of the scraping had a human-in-the-loop (i.e. myself) monitoring the scraper navigating the page, to ensure data retrieval quality. Occasionally, due to an unforeseen pop-up or a nested html element, I had to scroll the automated Chrome browser in the right direction so that it would carry on retrieving new job post URLs. However, this means that some job posts may have been overlooked. This shouldn&amp;rsquo;t pose a significant problem, since I was sampling from the much larger number of total available job posts, but the sampling method&amp;rsquo;s randomness is reliant on the order in which Indeed presents results.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Searching by title&lt;/strong&gt; - since we&amp;rsquo;re relying on Indeed.co.uk&amp;rsquo;s internal search functions, I specifically chose to input the text &amp;ldquo;title:(software, developer)&amp;rdquo;- this would only bring up a job if it contained either or both of those 2 words in any order. This would allow as well for examples such as &amp;ldquo;Software &amp;amp; Architecture Developer&amp;rdquo; and other valid variations. I realise of course that this would exclude perfectly valid choices from our selection, but I decided to prioritise sample purity foremost.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
